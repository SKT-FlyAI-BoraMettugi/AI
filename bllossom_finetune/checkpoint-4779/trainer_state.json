{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 4779,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006277463904582548,
      "grad_norm": 0.932168185710907,
      "learning_rate": 0.00019966520192508892,
      "loss": 2.9256,
      "step": 10
    },
    {
      "epoch": 0.012554927809165096,
      "grad_norm": 0.7231044173240662,
      "learning_rate": 0.0001992467043314501,
      "loss": 1.9155,
      "step": 20
    },
    {
      "epoch": 0.018832391713747645,
      "grad_norm": 0.7482074499130249,
      "learning_rate": 0.00019882820673781127,
      "loss": 1.7249,
      "step": 30
    },
    {
      "epoch": 0.025109855618330193,
      "grad_norm": 0.6125890612602234,
      "learning_rate": 0.00019840970914417242,
      "loss": 1.3715,
      "step": 40
    },
    {
      "epoch": 0.031387319522912745,
      "grad_norm": 0.5538865923881531,
      "learning_rate": 0.00019799121155053358,
      "loss": 1.4044,
      "step": 50
    },
    {
      "epoch": 0.03766478342749529,
      "grad_norm": 0.49625977873802185,
      "learning_rate": 0.00019757271395689477,
      "loss": 1.3989,
      "step": 60
    },
    {
      "epoch": 0.04394224733207784,
      "grad_norm": 0.4827761650085449,
      "learning_rate": 0.00019715421636325593,
      "loss": 1.4428,
      "step": 70
    },
    {
      "epoch": 0.050219711236660386,
      "grad_norm": 0.4940015971660614,
      "learning_rate": 0.0001967357187696171,
      "loss": 1.3618,
      "step": 80
    },
    {
      "epoch": 0.05649717514124294,
      "grad_norm": 0.5945996642112732,
      "learning_rate": 0.00019631722117597825,
      "loss": 1.363,
      "step": 90
    },
    {
      "epoch": 0.06277463904582549,
      "grad_norm": 0.553776741027832,
      "learning_rate": 0.0001958987235823394,
      "loss": 1.4174,
      "step": 100
    },
    {
      "epoch": 0.06905210295040803,
      "grad_norm": 0.5640967488288879,
      "learning_rate": 0.00019548022598870056,
      "loss": 1.3061,
      "step": 110
    },
    {
      "epoch": 0.07532956685499058,
      "grad_norm": 0.5838500261306763,
      "learning_rate": 0.00019506172839506175,
      "loss": 1.2411,
      "step": 120
    },
    {
      "epoch": 0.08160703075957314,
      "grad_norm": 0.4767564833164215,
      "learning_rate": 0.0001946432308014229,
      "loss": 1.2989,
      "step": 130
    },
    {
      "epoch": 0.08788449466415568,
      "grad_norm": 0.5223031044006348,
      "learning_rate": 0.00019422473320778407,
      "loss": 1.342,
      "step": 140
    },
    {
      "epoch": 0.09416195856873823,
      "grad_norm": 0.5819492936134338,
      "learning_rate": 0.00019380623561414523,
      "loss": 1.2332,
      "step": 150
    },
    {
      "epoch": 0.10043942247332077,
      "grad_norm": 0.5224732756614685,
      "learning_rate": 0.00019338773802050639,
      "loss": 1.335,
      "step": 160
    },
    {
      "epoch": 0.10671688637790333,
      "grad_norm": 0.5602819323539734,
      "learning_rate": 0.00019296924042686754,
      "loss": 1.2768,
      "step": 170
    },
    {
      "epoch": 0.11299435028248588,
      "grad_norm": 0.5439789891242981,
      "learning_rate": 0.0001925507428332287,
      "loss": 1.3653,
      "step": 180
    },
    {
      "epoch": 0.11927181418706842,
      "grad_norm": 0.5691562294960022,
      "learning_rate": 0.0001921322452395899,
      "loss": 1.1397,
      "step": 190
    },
    {
      "epoch": 0.12554927809165098,
      "grad_norm": 0.5623987913131714,
      "learning_rate": 0.00019171374764595105,
      "loss": 1.327,
      "step": 200
    },
    {
      "epoch": 0.1318267419962335,
      "grad_norm": 0.4990025758743286,
      "learning_rate": 0.0001912952500523122,
      "loss": 1.2959,
      "step": 210
    },
    {
      "epoch": 0.13810420590081607,
      "grad_norm": 0.5869375467300415,
      "learning_rate": 0.0001908767524586734,
      "loss": 1.3481,
      "step": 220
    },
    {
      "epoch": 0.14438166980539863,
      "grad_norm": 0.7071928977966309,
      "learning_rate": 0.00019045825486503452,
      "loss": 1.3288,
      "step": 230
    },
    {
      "epoch": 0.15065913370998116,
      "grad_norm": 0.5487568974494934,
      "learning_rate": 0.00019003975727139568,
      "loss": 1.3137,
      "step": 240
    },
    {
      "epoch": 0.15693659761456372,
      "grad_norm": 0.5254260897636414,
      "learning_rate": 0.00018962125967775687,
      "loss": 1.2077,
      "step": 250
    },
    {
      "epoch": 0.16321406151914628,
      "grad_norm": 0.6355239748954773,
      "learning_rate": 0.00018920276208411803,
      "loss": 1.3222,
      "step": 260
    },
    {
      "epoch": 0.1694915254237288,
      "grad_norm": 0.6866852045059204,
      "learning_rate": 0.0001887842644904792,
      "loss": 1.2311,
      "step": 270
    },
    {
      "epoch": 0.17576898932831136,
      "grad_norm": 0.5490036010742188,
      "learning_rate": 0.00018836576689684035,
      "loss": 1.1622,
      "step": 280
    },
    {
      "epoch": 0.18204645323289392,
      "grad_norm": 0.606478750705719,
      "learning_rate": 0.00018794726930320153,
      "loss": 1.2302,
      "step": 290
    },
    {
      "epoch": 0.18832391713747645,
      "grad_norm": 0.5532550811767578,
      "learning_rate": 0.0001875287717095627,
      "loss": 1.2107,
      "step": 300
    },
    {
      "epoch": 0.194601381042059,
      "grad_norm": 0.5585848093032837,
      "learning_rate": 0.00018711027411592382,
      "loss": 1.3093,
      "step": 310
    },
    {
      "epoch": 0.20087884494664154,
      "grad_norm": 0.5987712144851685,
      "learning_rate": 0.000186691776522285,
      "loss": 1.2486,
      "step": 320
    },
    {
      "epoch": 0.2071563088512241,
      "grad_norm": 0.5747607946395874,
      "learning_rate": 0.00018627327892864617,
      "loss": 1.1906,
      "step": 330
    },
    {
      "epoch": 0.21343377275580666,
      "grad_norm": 0.5857958793640137,
      "learning_rate": 0.00018585478133500733,
      "loss": 1.3232,
      "step": 340
    },
    {
      "epoch": 0.2197112366603892,
      "grad_norm": 0.6156118512153625,
      "learning_rate": 0.00018543628374136849,
      "loss": 1.3076,
      "step": 350
    },
    {
      "epoch": 0.22598870056497175,
      "grad_norm": 0.529983401298523,
      "learning_rate": 0.00018501778614772967,
      "loss": 1.1786,
      "step": 360
    },
    {
      "epoch": 0.2322661644695543,
      "grad_norm": 0.6434121131896973,
      "learning_rate": 0.00018459928855409083,
      "loss": 1.1651,
      "step": 370
    },
    {
      "epoch": 0.23854362837413684,
      "grad_norm": 0.6037746071815491,
      "learning_rate": 0.000184180790960452,
      "loss": 1.2013,
      "step": 380
    },
    {
      "epoch": 0.2448210922787194,
      "grad_norm": 0.5821081399917603,
      "learning_rate": 0.00018376229336681315,
      "loss": 1.2934,
      "step": 390
    },
    {
      "epoch": 0.25109855618330196,
      "grad_norm": 0.6177308559417725,
      "learning_rate": 0.0001833437957731743,
      "loss": 1.264,
      "step": 400
    },
    {
      "epoch": 0.2573760200878845,
      "grad_norm": 0.6003419756889343,
      "learning_rate": 0.00018292529817953547,
      "loss": 1.2523,
      "step": 410
    },
    {
      "epoch": 0.263653483992467,
      "grad_norm": 0.5575363636016846,
      "learning_rate": 0.00018250680058589665,
      "loss": 1.2351,
      "step": 420
    },
    {
      "epoch": 0.2699309478970496,
      "grad_norm": 0.6556302309036255,
      "learning_rate": 0.0001820883029922578,
      "loss": 1.3164,
      "step": 430
    },
    {
      "epoch": 0.27620841180163214,
      "grad_norm": 0.5866472125053406,
      "learning_rate": 0.00018166980539861897,
      "loss": 1.1167,
      "step": 440
    },
    {
      "epoch": 0.2824858757062147,
      "grad_norm": 0.5813003778457642,
      "learning_rate": 0.00018125130780498013,
      "loss": 1.3218,
      "step": 450
    },
    {
      "epoch": 0.28876333961079725,
      "grad_norm": 0.5293742418289185,
      "learning_rate": 0.0001808328102113413,
      "loss": 1.1309,
      "step": 460
    },
    {
      "epoch": 0.2950408035153798,
      "grad_norm": 0.5999680161476135,
      "learning_rate": 0.00018041431261770245,
      "loss": 1.1407,
      "step": 470
    },
    {
      "epoch": 0.3013182674199623,
      "grad_norm": 0.6984972357749939,
      "learning_rate": 0.0001799958150240636,
      "loss": 1.172,
      "step": 480
    },
    {
      "epoch": 0.3075957313245449,
      "grad_norm": 0.5807093381881714,
      "learning_rate": 0.0001795773174304248,
      "loss": 1.1185,
      "step": 490
    },
    {
      "epoch": 0.31387319522912743,
      "grad_norm": 0.6095670461654663,
      "learning_rate": 0.00017915881983678595,
      "loss": 1.2552,
      "step": 500
    },
    {
      "epoch": 0.32015065913371,
      "grad_norm": 0.6176935434341431,
      "learning_rate": 0.0001787403222431471,
      "loss": 1.1024,
      "step": 510
    },
    {
      "epoch": 0.32642812303829255,
      "grad_norm": 0.6117128133773804,
      "learning_rate": 0.0001783218246495083,
      "loss": 1.1322,
      "step": 520
    },
    {
      "epoch": 0.33270558694287505,
      "grad_norm": 0.5836730003356934,
      "learning_rate": 0.00017790332705586943,
      "loss": 1.1881,
      "step": 530
    },
    {
      "epoch": 0.3389830508474576,
      "grad_norm": 0.6919256448745728,
      "learning_rate": 0.0001774848294622306,
      "loss": 1.1781,
      "step": 540
    },
    {
      "epoch": 0.34526051475204017,
      "grad_norm": 0.6048715710639954,
      "learning_rate": 0.00017706633186859177,
      "loss": 1.2875,
      "step": 550
    },
    {
      "epoch": 0.35153797865662273,
      "grad_norm": 0.6428632736206055,
      "learning_rate": 0.00017664783427495293,
      "loss": 1.1812,
      "step": 560
    },
    {
      "epoch": 0.3578154425612053,
      "grad_norm": 0.5968446135520935,
      "learning_rate": 0.0001762293366813141,
      "loss": 1.0925,
      "step": 570
    },
    {
      "epoch": 0.36409290646578785,
      "grad_norm": 0.6082137823104858,
      "learning_rate": 0.00017581083908767525,
      "loss": 1.1327,
      "step": 580
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 0.658845067024231,
      "learning_rate": 0.00017539234149403644,
      "loss": 1.2199,
      "step": 590
    },
    {
      "epoch": 0.3766478342749529,
      "grad_norm": 0.5680555105209351,
      "learning_rate": 0.00017497384390039757,
      "loss": 1.1641,
      "step": 600
    },
    {
      "epoch": 0.38292529817953547,
      "grad_norm": 0.6168239116668701,
      "learning_rate": 0.00017455534630675873,
      "loss": 1.1907,
      "step": 610
    },
    {
      "epoch": 0.389202762084118,
      "grad_norm": 0.6627384424209595,
      "learning_rate": 0.0001741368487131199,
      "loss": 1.2232,
      "step": 620
    },
    {
      "epoch": 0.3954802259887006,
      "grad_norm": 0.6467877626419067,
      "learning_rate": 0.00017371835111948107,
      "loss": 1.1162,
      "step": 630
    },
    {
      "epoch": 0.4017576898932831,
      "grad_norm": 0.5411285758018494,
      "learning_rate": 0.00017329985352584223,
      "loss": 1.0502,
      "step": 640
    },
    {
      "epoch": 0.40803515379786565,
      "grad_norm": 0.6716124415397644,
      "learning_rate": 0.00017288135593220342,
      "loss": 1.1589,
      "step": 650
    },
    {
      "epoch": 0.4143126177024482,
      "grad_norm": 0.750100314617157,
      "learning_rate": 0.00017246285833856457,
      "loss": 1.1205,
      "step": 660
    },
    {
      "epoch": 0.42059008160703076,
      "grad_norm": 0.6240184307098389,
      "learning_rate": 0.0001720443607449257,
      "loss": 1.1092,
      "step": 670
    },
    {
      "epoch": 0.4268675455116133,
      "grad_norm": 0.7163110971450806,
      "learning_rate": 0.0001716258631512869,
      "loss": 1.2119,
      "step": 680
    },
    {
      "epoch": 0.4331450094161959,
      "grad_norm": 0.6634250283241272,
      "learning_rate": 0.00017120736555764805,
      "loss": 1.1628,
      "step": 690
    },
    {
      "epoch": 0.4394224733207784,
      "grad_norm": 0.5920068621635437,
      "learning_rate": 0.0001707888679640092,
      "loss": 1.181,
      "step": 700
    },
    {
      "epoch": 0.44569993722536094,
      "grad_norm": 0.685039758682251,
      "learning_rate": 0.00017037037037037037,
      "loss": 1.1402,
      "step": 710
    },
    {
      "epoch": 0.4519774011299435,
      "grad_norm": 0.618159294128418,
      "learning_rate": 0.00016995187277673156,
      "loss": 1.1517,
      "step": 720
    },
    {
      "epoch": 0.45825486503452606,
      "grad_norm": 0.6547849774360657,
      "learning_rate": 0.00016953337518309271,
      "loss": 1.112,
      "step": 730
    },
    {
      "epoch": 0.4645323289391086,
      "grad_norm": 0.5844735503196716,
      "learning_rate": 0.00016911487758945387,
      "loss": 1.2382,
      "step": 740
    },
    {
      "epoch": 0.4708097928436911,
      "grad_norm": 0.6468414664268494,
      "learning_rate": 0.00016869637999581503,
      "loss": 1.1144,
      "step": 750
    },
    {
      "epoch": 0.4770872567482737,
      "grad_norm": 0.6308430433273315,
      "learning_rate": 0.0001682778824021762,
      "loss": 1.2069,
      "step": 760
    },
    {
      "epoch": 0.48336472065285624,
      "grad_norm": 0.6815651655197144,
      "learning_rate": 0.00016785938480853735,
      "loss": 1.1049,
      "step": 770
    },
    {
      "epoch": 0.4896421845574388,
      "grad_norm": 0.5753756165504456,
      "learning_rate": 0.00016744088721489854,
      "loss": 1.1135,
      "step": 780
    },
    {
      "epoch": 0.49591964846202136,
      "grad_norm": 0.51607745885849,
      "learning_rate": 0.0001670223896212597,
      "loss": 1.1865,
      "step": 790
    },
    {
      "epoch": 0.5021971123666039,
      "grad_norm": 0.5711470246315002,
      "learning_rate": 0.00016660389202762085,
      "loss": 1.0901,
      "step": 800
    },
    {
      "epoch": 0.5084745762711864,
      "grad_norm": 0.5754309892654419,
      "learning_rate": 0.000166185394433982,
      "loss": 1.1554,
      "step": 810
    },
    {
      "epoch": 0.514752040175769,
      "grad_norm": 0.6652283072471619,
      "learning_rate": 0.00016576689684034317,
      "loss": 1.056,
      "step": 820
    },
    {
      "epoch": 0.5210295040803515,
      "grad_norm": 0.6516441702842712,
      "learning_rate": 0.00016534839924670433,
      "loss": 1.1357,
      "step": 830
    },
    {
      "epoch": 0.527306967984934,
      "grad_norm": 0.670681893825531,
      "learning_rate": 0.0001649299016530655,
      "loss": 1.1091,
      "step": 840
    },
    {
      "epoch": 0.5335844318895167,
      "grad_norm": 0.6143217086791992,
      "learning_rate": 0.00016451140405942668,
      "loss": 1.118,
      "step": 850
    },
    {
      "epoch": 0.5398618957940992,
      "grad_norm": 0.6466380953788757,
      "learning_rate": 0.00016409290646578783,
      "loss": 1.1217,
      "step": 860
    },
    {
      "epoch": 0.5461393596986818,
      "grad_norm": 0.7308236360549927,
      "learning_rate": 0.000163674408872149,
      "loss": 1.1323,
      "step": 870
    },
    {
      "epoch": 0.5524168236032643,
      "grad_norm": 0.6125099658966064,
      "learning_rate": 0.00016325591127851015,
      "loss": 1.1789,
      "step": 880
    },
    {
      "epoch": 0.5586942875078468,
      "grad_norm": 0.676082193851471,
      "learning_rate": 0.0001628374136848713,
      "loss": 1.1329,
      "step": 890
    },
    {
      "epoch": 0.5649717514124294,
      "grad_norm": 0.5871046781539917,
      "learning_rate": 0.00016241891609123247,
      "loss": 1.0751,
      "step": 900
    },
    {
      "epoch": 0.5712492153170119,
      "grad_norm": 0.635615885257721,
      "learning_rate": 0.00016200041849759363,
      "loss": 1.1082,
      "step": 910
    },
    {
      "epoch": 0.5775266792215945,
      "grad_norm": 0.6593565344810486,
      "learning_rate": 0.00016158192090395482,
      "loss": 1.0874,
      "step": 920
    },
    {
      "epoch": 0.583804143126177,
      "grad_norm": 0.6341730356216431,
      "learning_rate": 0.00016116342331031597,
      "loss": 1.0758,
      "step": 930
    },
    {
      "epoch": 0.5900816070307596,
      "grad_norm": 0.6925804615020752,
      "learning_rate": 0.00016074492571667713,
      "loss": 1.2282,
      "step": 940
    },
    {
      "epoch": 0.5963590709353421,
      "grad_norm": 0.6672784686088562,
      "learning_rate": 0.00016032642812303832,
      "loss": 1.0888,
      "step": 950
    },
    {
      "epoch": 0.6026365348399246,
      "grad_norm": 0.6547020673751831,
      "learning_rate": 0.00015990793052939948,
      "loss": 1.0958,
      "step": 960
    },
    {
      "epoch": 0.6089139987445072,
      "grad_norm": 0.6606735587120056,
      "learning_rate": 0.0001594894329357606,
      "loss": 1.2696,
      "step": 970
    },
    {
      "epoch": 0.6151914626490897,
      "grad_norm": 0.6772971749305725,
      "learning_rate": 0.0001590709353421218,
      "loss": 1.118,
      "step": 980
    },
    {
      "epoch": 0.6214689265536724,
      "grad_norm": 0.6363801956176758,
      "learning_rate": 0.00015865243774848295,
      "loss": 1.166,
      "step": 990
    },
    {
      "epoch": 0.6277463904582549,
      "grad_norm": 0.6266388297080994,
      "learning_rate": 0.0001582339401548441,
      "loss": 1.1045,
      "step": 1000
    },
    {
      "epoch": 0.6340238543628374,
      "grad_norm": 0.6218175888061523,
      "learning_rate": 0.00015781544256120527,
      "loss": 1.0408,
      "step": 1010
    },
    {
      "epoch": 0.64030131826742,
      "grad_norm": 0.5792235732078552,
      "learning_rate": 0.00015739694496756646,
      "loss": 1.104,
      "step": 1020
    },
    {
      "epoch": 0.6465787821720025,
      "grad_norm": 0.6294731497764587,
      "learning_rate": 0.00015697844737392762,
      "loss": 1.0829,
      "step": 1030
    },
    {
      "epoch": 0.6528562460765851,
      "grad_norm": 0.7403737902641296,
      "learning_rate": 0.00015655994978028875,
      "loss": 1.1151,
      "step": 1040
    },
    {
      "epoch": 0.6591337099811676,
      "grad_norm": 0.6450133919715881,
      "learning_rate": 0.00015614145218664994,
      "loss": 1.0481,
      "step": 1050
    },
    {
      "epoch": 0.6654111738857501,
      "grad_norm": 0.7045389413833618,
      "learning_rate": 0.0001557229545930111,
      "loss": 1.1781,
      "step": 1060
    },
    {
      "epoch": 0.6716886377903327,
      "grad_norm": 0.6846441626548767,
      "learning_rate": 0.00015530445699937225,
      "loss": 1.1366,
      "step": 1070
    },
    {
      "epoch": 0.6779661016949152,
      "grad_norm": 0.7629581093788147,
      "learning_rate": 0.00015488595940573344,
      "loss": 1.1758,
      "step": 1080
    },
    {
      "epoch": 0.6842435655994978,
      "grad_norm": 0.59344482421875,
      "learning_rate": 0.0001544674618120946,
      "loss": 0.9343,
      "step": 1090
    },
    {
      "epoch": 0.6905210295040803,
      "grad_norm": 0.7283827662467957,
      "learning_rate": 0.00015404896421845576,
      "loss": 1.0856,
      "step": 1100
    },
    {
      "epoch": 0.696798493408663,
      "grad_norm": 0.6444827914237976,
      "learning_rate": 0.00015363046662481692,
      "loss": 1.2752,
      "step": 1110
    },
    {
      "epoch": 0.7030759573132455,
      "grad_norm": 0.6085330843925476,
      "learning_rate": 0.00015321196903117807,
      "loss": 1.2076,
      "step": 1120
    },
    {
      "epoch": 0.709353421217828,
      "grad_norm": 0.6553382277488708,
      "learning_rate": 0.00015279347143753923,
      "loss": 1.0372,
      "step": 1130
    },
    {
      "epoch": 0.7156308851224106,
      "grad_norm": 0.7848677635192871,
      "learning_rate": 0.0001523749738439004,
      "loss": 1.1661,
      "step": 1140
    },
    {
      "epoch": 0.7219083490269931,
      "grad_norm": 0.7708779573440552,
      "learning_rate": 0.00015195647625026158,
      "loss": 1.068,
      "step": 1150
    },
    {
      "epoch": 0.7281858129315757,
      "grad_norm": 0.7204033136367798,
      "learning_rate": 0.00015153797865662274,
      "loss": 1.0772,
      "step": 1160
    },
    {
      "epoch": 0.7344632768361582,
      "grad_norm": 0.7171818017959595,
      "learning_rate": 0.0001511194810629839,
      "loss": 1.0819,
      "step": 1170
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 0.6792163252830505,
      "learning_rate": 0.00015070098346934508,
      "loss": 1.0478,
      "step": 1180
    },
    {
      "epoch": 0.7470182046453233,
      "grad_norm": 0.7011916041374207,
      "learning_rate": 0.00015028248587570621,
      "loss": 1.0659,
      "step": 1190
    },
    {
      "epoch": 0.7532956685499058,
      "grad_norm": 0.771087110042572,
      "learning_rate": 0.00014986398828206737,
      "loss": 1.0572,
      "step": 1200
    },
    {
      "epoch": 0.7595731324544884,
      "grad_norm": 0.7213941216468811,
      "learning_rate": 0.00014944549068842856,
      "loss": 1.1349,
      "step": 1210
    },
    {
      "epoch": 0.7658505963590709,
      "grad_norm": 0.7402423620223999,
      "learning_rate": 0.00014902699309478972,
      "loss": 1.0439,
      "step": 1220
    },
    {
      "epoch": 0.7721280602636534,
      "grad_norm": 0.7291733622550964,
      "learning_rate": 0.00014860849550115088,
      "loss": 0.9946,
      "step": 1230
    },
    {
      "epoch": 0.778405524168236,
      "grad_norm": 0.6595091223716736,
      "learning_rate": 0.00014818999790751204,
      "loss": 1.1717,
      "step": 1240
    },
    {
      "epoch": 0.7846829880728186,
      "grad_norm": 0.7239333987236023,
      "learning_rate": 0.00014777150031387322,
      "loss": 1.1148,
      "step": 1250
    },
    {
      "epoch": 0.7909604519774012,
      "grad_norm": 0.6787055730819702,
      "learning_rate": 0.00014735300272023435,
      "loss": 1.1886,
      "step": 1260
    },
    {
      "epoch": 0.7972379158819837,
      "grad_norm": 0.6657481789588928,
      "learning_rate": 0.0001469345051265955,
      "loss": 1.0916,
      "step": 1270
    },
    {
      "epoch": 0.8035153797865662,
      "grad_norm": 0.6373681426048279,
      "learning_rate": 0.0001465160075329567,
      "loss": 1.0502,
      "step": 1280
    },
    {
      "epoch": 0.8097928436911488,
      "grad_norm": 0.7223085761070251,
      "learning_rate": 0.00014609750993931786,
      "loss": 1.0622,
      "step": 1290
    },
    {
      "epoch": 0.8160703075957313,
      "grad_norm": 0.7469496130943298,
      "learning_rate": 0.00014567901234567902,
      "loss": 1.0787,
      "step": 1300
    },
    {
      "epoch": 0.8223477715003139,
      "grad_norm": 0.7159574627876282,
      "learning_rate": 0.0001452605147520402,
      "loss": 1.1834,
      "step": 1310
    },
    {
      "epoch": 0.8286252354048964,
      "grad_norm": 0.644140362739563,
      "learning_rate": 0.00014484201715840136,
      "loss": 1.0397,
      "step": 1320
    },
    {
      "epoch": 0.834902699309479,
      "grad_norm": 0.7259247899055481,
      "learning_rate": 0.0001444235195647625,
      "loss": 1.1355,
      "step": 1330
    },
    {
      "epoch": 0.8411801632140615,
      "grad_norm": 0.6653093099594116,
      "learning_rate": 0.00014400502197112368,
      "loss": 1.0427,
      "step": 1340
    },
    {
      "epoch": 0.847457627118644,
      "grad_norm": 0.7973365187644958,
      "learning_rate": 0.00014358652437748484,
      "loss": 1.1029,
      "step": 1350
    },
    {
      "epoch": 0.8537350910232266,
      "grad_norm": 0.6979200839996338,
      "learning_rate": 0.000143168026783846,
      "loss": 1.1607,
      "step": 1360
    },
    {
      "epoch": 0.8600125549278091,
      "grad_norm": 0.6326670050621033,
      "learning_rate": 0.00014274952919020716,
      "loss": 1.045,
      "step": 1370
    },
    {
      "epoch": 0.8662900188323918,
      "grad_norm": 0.625702440738678,
      "learning_rate": 0.00014233103159656834,
      "loss": 1.0339,
      "step": 1380
    },
    {
      "epoch": 0.8725674827369743,
      "grad_norm": 0.6416098475456238,
      "learning_rate": 0.0001419125340029295,
      "loss": 1.0018,
      "step": 1390
    },
    {
      "epoch": 0.8788449466415568,
      "grad_norm": 0.7704829573631287,
      "learning_rate": 0.00014149403640929063,
      "loss": 1.2115,
      "step": 1400
    },
    {
      "epoch": 0.8851224105461394,
      "grad_norm": 0.7153869271278381,
      "learning_rate": 0.00014107553881565182,
      "loss": 1.0938,
      "step": 1410
    },
    {
      "epoch": 0.8913998744507219,
      "grad_norm": 0.6864328980445862,
      "learning_rate": 0.00014065704122201298,
      "loss": 1.0394,
      "step": 1420
    },
    {
      "epoch": 0.8976773383553045,
      "grad_norm": 0.654288649559021,
      "learning_rate": 0.00014023854362837414,
      "loss": 1.1938,
      "step": 1430
    },
    {
      "epoch": 0.903954802259887,
      "grad_norm": 0.7491419315338135,
      "learning_rate": 0.0001398200460347353,
      "loss": 1.1091,
      "step": 1440
    },
    {
      "epoch": 0.9102322661644695,
      "grad_norm": 0.6868979334831238,
      "learning_rate": 0.00013940154844109648,
      "loss": 0.9598,
      "step": 1450
    },
    {
      "epoch": 0.9165097300690521,
      "grad_norm": 0.6900007724761963,
      "learning_rate": 0.00013898305084745764,
      "loss": 1.0869,
      "step": 1460
    },
    {
      "epoch": 0.9227871939736346,
      "grad_norm": 0.7423304319381714,
      "learning_rate": 0.0001385645532538188,
      "loss": 1.0446,
      "step": 1470
    },
    {
      "epoch": 0.9290646578782172,
      "grad_norm": 0.5945031642913818,
      "learning_rate": 0.00013814605566017996,
      "loss": 1.0632,
      "step": 1480
    },
    {
      "epoch": 0.9353421217827997,
      "grad_norm": 0.7075721621513367,
      "learning_rate": 0.00013772755806654112,
      "loss": 0.9744,
      "step": 1490
    },
    {
      "epoch": 0.9416195856873822,
      "grad_norm": 0.5863187909126282,
      "learning_rate": 0.00013730906047290228,
      "loss": 1.103,
      "step": 1500
    },
    {
      "epoch": 0.9478970495919649,
      "grad_norm": 0.7364603281021118,
      "learning_rate": 0.00013689056287926346,
      "loss": 1.1258,
      "step": 1510
    },
    {
      "epoch": 0.9541745134965474,
      "grad_norm": 0.7016408443450928,
      "learning_rate": 0.00013647206528562462,
      "loss": 1.0274,
      "step": 1520
    },
    {
      "epoch": 0.96045197740113,
      "grad_norm": 0.6275646090507507,
      "learning_rate": 0.00013605356769198578,
      "loss": 1.0329,
      "step": 1530
    },
    {
      "epoch": 0.9667294413057125,
      "grad_norm": 0.6753239631652832,
      "learning_rate": 0.00013563507009834694,
      "loss": 0.9847,
      "step": 1540
    },
    {
      "epoch": 0.9730069052102951,
      "grad_norm": 0.6707074642181396,
      "learning_rate": 0.0001352165725047081,
      "loss": 0.9565,
      "step": 1550
    },
    {
      "epoch": 0.9792843691148776,
      "grad_norm": 0.9146836996078491,
      "learning_rate": 0.00013479807491106926,
      "loss": 1.0973,
      "step": 1560
    },
    {
      "epoch": 0.9855618330194601,
      "grad_norm": 0.659652829170227,
      "learning_rate": 0.00013437957731743042,
      "loss": 0.9848,
      "step": 1570
    },
    {
      "epoch": 0.9918392969240427,
      "grad_norm": 0.7146149277687073,
      "learning_rate": 0.0001339610797237916,
      "loss": 1.0156,
      "step": 1580
    },
    {
      "epoch": 0.9981167608286252,
      "grad_norm": 0.6854270696640015,
      "learning_rate": 0.00013354258213015276,
      "loss": 1.008,
      "step": 1590
    },
    {
      "epoch": 1.0043942247332078,
      "grad_norm": 0.6003192067146301,
      "learning_rate": 0.00013312408453651392,
      "loss": 0.9894,
      "step": 1600
    },
    {
      "epoch": 1.0106716886377902,
      "grad_norm": 0.7241529226303101,
      "learning_rate": 0.0001327055869428751,
      "loss": 1.0649,
      "step": 1610
    },
    {
      "epoch": 1.0169491525423728,
      "grad_norm": 0.6407097578048706,
      "learning_rate": 0.00013228708934923624,
      "loss": 1.0458,
      "step": 1620
    },
    {
      "epoch": 1.0232266164469555,
      "grad_norm": 0.6208394765853882,
      "learning_rate": 0.0001318685917555974,
      "loss": 1.0023,
      "step": 1630
    },
    {
      "epoch": 1.029504080351538,
      "grad_norm": 0.6245283484458923,
      "learning_rate": 0.00013145009416195858,
      "loss": 0.9569,
      "step": 1640
    },
    {
      "epoch": 1.0357815442561205,
      "grad_norm": 0.7469456195831299,
      "learning_rate": 0.00013103159656831974,
      "loss": 1.0283,
      "step": 1650
    },
    {
      "epoch": 1.042059008160703,
      "grad_norm": 0.6283392310142517,
      "learning_rate": 0.0001306130989746809,
      "loss": 1.0558,
      "step": 1660
    },
    {
      "epoch": 1.0483364720652857,
      "grad_norm": 0.6579342484474182,
      "learning_rate": 0.00013019460138104206,
      "loss": 1.0348,
      "step": 1670
    },
    {
      "epoch": 1.054613935969868,
      "grad_norm": 0.6421488523483276,
      "learning_rate": 0.00012977610378740324,
      "loss": 1.0244,
      "step": 1680
    },
    {
      "epoch": 1.0608913998744507,
      "grad_norm": 0.6579471826553345,
      "learning_rate": 0.0001293576061937644,
      "loss": 0.9733,
      "step": 1690
    },
    {
      "epoch": 1.0671688637790333,
      "grad_norm": 0.643628716468811,
      "learning_rate": 0.00012893910860012554,
      "loss": 0.963,
      "step": 1700
    },
    {
      "epoch": 1.073446327683616,
      "grad_norm": 0.6279364228248596,
      "learning_rate": 0.00012852061100648672,
      "loss": 0.9363,
      "step": 1710
    },
    {
      "epoch": 1.0797237915881983,
      "grad_norm": 0.7169364094734192,
      "learning_rate": 0.00012810211341284788,
      "loss": 0.9956,
      "step": 1720
    },
    {
      "epoch": 1.086001255492781,
      "grad_norm": 0.651616632938385,
      "learning_rate": 0.00012768361581920904,
      "loss": 1.0894,
      "step": 1730
    },
    {
      "epoch": 1.0922787193973635,
      "grad_norm": 0.7480227947235107,
      "learning_rate": 0.00012726511822557023,
      "loss": 1.0015,
      "step": 1740
    },
    {
      "epoch": 1.098556183301946,
      "grad_norm": 0.7078272104263306,
      "learning_rate": 0.00012684662063193138,
      "loss": 0.9603,
      "step": 1750
    },
    {
      "epoch": 1.1048336472065285,
      "grad_norm": 0.7698292136192322,
      "learning_rate": 0.00012642812303829254,
      "loss": 1.0975,
      "step": 1760
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.7660356163978577,
      "learning_rate": 0.0001260096254446537,
      "loss": 1.0254,
      "step": 1770
    },
    {
      "epoch": 1.1173885750156938,
      "grad_norm": 0.7337697744369507,
      "learning_rate": 0.00012559112785101486,
      "loss": 0.9749,
      "step": 1780
    },
    {
      "epoch": 1.1236660389202762,
      "grad_norm": 0.6530495882034302,
      "learning_rate": 0.00012517263025737602,
      "loss": 0.9469,
      "step": 1790
    },
    {
      "epoch": 1.1299435028248588,
      "grad_norm": 0.7335605621337891,
      "learning_rate": 0.00012475413266373718,
      "loss": 1.0395,
      "step": 1800
    },
    {
      "epoch": 1.1362209667294414,
      "grad_norm": 0.7935880422592163,
      "learning_rate": 0.00012433563507009836,
      "loss": 0.9814,
      "step": 1810
    },
    {
      "epoch": 1.1424984306340238,
      "grad_norm": 0.6367689967155457,
      "learning_rate": 0.00012391713747645952,
      "loss": 1.1152,
      "step": 1820
    },
    {
      "epoch": 1.1487758945386064,
      "grad_norm": 0.5988656282424927,
      "learning_rate": 0.00012349863988282068,
      "loss": 1.0173,
      "step": 1830
    },
    {
      "epoch": 1.155053358443189,
      "grad_norm": 0.6528387665748596,
      "learning_rate": 0.00012308014228918184,
      "loss": 1.0382,
      "step": 1840
    },
    {
      "epoch": 1.1613308223477714,
      "grad_norm": 0.7551480531692505,
      "learning_rate": 0.000122661644695543,
      "loss": 1.0975,
      "step": 1850
    },
    {
      "epoch": 1.167608286252354,
      "grad_norm": 0.6961734294891357,
      "learning_rate": 0.00012224314710190416,
      "loss": 1.0291,
      "step": 1860
    },
    {
      "epoch": 1.1738857501569366,
      "grad_norm": 0.6932178139686584,
      "learning_rate": 0.00012182464950826533,
      "loss": 1.0235,
      "step": 1870
    },
    {
      "epoch": 1.180163214061519,
      "grad_norm": 0.6554912328720093,
      "learning_rate": 0.0001214061519146265,
      "loss": 0.9901,
      "step": 1880
    },
    {
      "epoch": 1.1864406779661016,
      "grad_norm": 0.8311054110527039,
      "learning_rate": 0.00012098765432098766,
      "loss": 1.1,
      "step": 1890
    },
    {
      "epoch": 1.1927181418706843,
      "grad_norm": 0.7572940587997437,
      "learning_rate": 0.00012056915672734884,
      "loss": 1.0725,
      "step": 1900
    },
    {
      "epoch": 1.1989956057752669,
      "grad_norm": 0.6659161448478699,
      "learning_rate": 0.00012015065913371,
      "loss": 1.0943,
      "step": 1910
    },
    {
      "epoch": 1.2052730696798493,
      "grad_norm": 0.760523796081543,
      "learning_rate": 0.00011973216154007114,
      "loss": 1.0631,
      "step": 1920
    },
    {
      "epoch": 1.2115505335844319,
      "grad_norm": 0.7421793341636658,
      "learning_rate": 0.00011931366394643231,
      "loss": 1.0091,
      "step": 1930
    },
    {
      "epoch": 1.2178279974890145,
      "grad_norm": 0.7063173651695251,
      "learning_rate": 0.00011889516635279347,
      "loss": 1.0146,
      "step": 1940
    },
    {
      "epoch": 1.2241054613935969,
      "grad_norm": 0.7936040163040161,
      "learning_rate": 0.00011847666875915464,
      "loss": 1.1205,
      "step": 1950
    },
    {
      "epoch": 1.2303829252981795,
      "grad_norm": 0.5886478424072266,
      "learning_rate": 0.0001180581711655158,
      "loss": 0.923,
      "step": 1960
    },
    {
      "epoch": 1.2366603892027621,
      "grad_norm": 0.6369995474815369,
      "learning_rate": 0.00011763967357187698,
      "loss": 1.0218,
      "step": 1970
    },
    {
      "epoch": 1.2429378531073447,
      "grad_norm": 0.7052714228630066,
      "learning_rate": 0.00011722117597823813,
      "loss": 0.9713,
      "step": 1980
    },
    {
      "epoch": 1.2492153170119271,
      "grad_norm": 0.5779129862785339,
      "learning_rate": 0.00011680267838459928,
      "loss": 0.9676,
      "step": 1990
    },
    {
      "epoch": 1.2554927809165097,
      "grad_norm": 0.6255979537963867,
      "learning_rate": 0.00011638418079096045,
      "loss": 1.0496,
      "step": 2000
    },
    {
      "epoch": 1.2617702448210923,
      "grad_norm": 0.7064706087112427,
      "learning_rate": 0.00011596568319732161,
      "loss": 1.0661,
      "step": 2010
    },
    {
      "epoch": 1.2680477087256747,
      "grad_norm": 0.683211088180542,
      "learning_rate": 0.00011554718560368278,
      "loss": 0.9842,
      "step": 2020
    },
    {
      "epoch": 1.2743251726302574,
      "grad_norm": 0.7057439684867859,
      "learning_rate": 0.00011512868801004396,
      "loss": 0.9978,
      "step": 2030
    },
    {
      "epoch": 1.28060263653484,
      "grad_norm": 0.627629280090332,
      "learning_rate": 0.00011471019041640511,
      "loss": 1.032,
      "step": 2040
    },
    {
      "epoch": 1.2868801004394226,
      "grad_norm": 0.6367796063423157,
      "learning_rate": 0.00011429169282276629,
      "loss": 0.9783,
      "step": 2050
    },
    {
      "epoch": 1.293157564344005,
      "grad_norm": 0.726061999797821,
      "learning_rate": 0.00011387319522912743,
      "loss": 0.9944,
      "step": 2060
    },
    {
      "epoch": 1.2994350282485876,
      "grad_norm": 0.7142391800880432,
      "learning_rate": 0.00011345469763548859,
      "loss": 1.0937,
      "step": 2070
    },
    {
      "epoch": 1.3057124921531702,
      "grad_norm": 0.6838685274124146,
      "learning_rate": 0.00011303620004184976,
      "loss": 1.0235,
      "step": 2080
    },
    {
      "epoch": 1.3119899560577526,
      "grad_norm": 0.732172429561615,
      "learning_rate": 0.00011261770244821092,
      "loss": 0.9669,
      "step": 2090
    },
    {
      "epoch": 1.3182674199623352,
      "grad_norm": 0.7668601274490356,
      "learning_rate": 0.0001121992048545721,
      "loss": 0.9712,
      "step": 2100
    },
    {
      "epoch": 1.3245448838669178,
      "grad_norm": 0.7525842785835266,
      "learning_rate": 0.00011178070726093325,
      "loss": 1.1188,
      "step": 2110
    },
    {
      "epoch": 1.3308223477715004,
      "grad_norm": 0.7587542533874512,
      "learning_rate": 0.00011136220966729443,
      "loss": 1.0777,
      "step": 2120
    },
    {
      "epoch": 1.3370998116760828,
      "grad_norm": 0.794150710105896,
      "learning_rate": 0.0001109437120736556,
      "loss": 1.0167,
      "step": 2130
    },
    {
      "epoch": 1.3433772755806654,
      "grad_norm": 0.7322328686714172,
      "learning_rate": 0.00011052521448001673,
      "loss": 0.9778,
      "step": 2140
    },
    {
      "epoch": 1.3496547394852478,
      "grad_norm": 0.6958417296409607,
      "learning_rate": 0.0001101067168863779,
      "loss": 0.8358,
      "step": 2150
    },
    {
      "epoch": 1.3559322033898304,
      "grad_norm": 0.6377500891685486,
      "learning_rate": 0.00010968821929273908,
      "loss": 0.9773,
      "step": 2160
    },
    {
      "epoch": 1.362209667294413,
      "grad_norm": 0.6809850931167603,
      "learning_rate": 0.00010926972169910023,
      "loss": 0.9628,
      "step": 2170
    },
    {
      "epoch": 1.3684871311989957,
      "grad_norm": 0.6270563006401062,
      "learning_rate": 0.00010885122410546141,
      "loss": 0.9903,
      "step": 2180
    },
    {
      "epoch": 1.3747645951035783,
      "grad_norm": 0.6804837584495544,
      "learning_rate": 0.00010843272651182257,
      "loss": 1.0514,
      "step": 2190
    },
    {
      "epoch": 1.3810420590081607,
      "grad_norm": 0.6313058137893677,
      "learning_rate": 0.00010801422891818374,
      "loss": 1.0643,
      "step": 2200
    },
    {
      "epoch": 1.3873195229127433,
      "grad_norm": 0.7516151666641235,
      "learning_rate": 0.00010759573132454488,
      "loss": 1.0973,
      "step": 2210
    },
    {
      "epoch": 1.3935969868173257,
      "grad_norm": 0.721291184425354,
      "learning_rate": 0.00010717723373090604,
      "loss": 1.0807,
      "step": 2220
    },
    {
      "epoch": 1.3998744507219083,
      "grad_norm": 0.7824544310569763,
      "learning_rate": 0.00010675873613726722,
      "loss": 1.0286,
      "step": 2230
    },
    {
      "epoch": 1.406151914626491,
      "grad_norm": 0.7278753519058228,
      "learning_rate": 0.00010634023854362837,
      "loss": 0.9777,
      "step": 2240
    },
    {
      "epoch": 1.4124293785310735,
      "grad_norm": 0.8517118692398071,
      "learning_rate": 0.00010592174094998955,
      "loss": 0.9735,
      "step": 2250
    },
    {
      "epoch": 1.418706842435656,
      "grad_norm": 0.7170505523681641,
      "learning_rate": 0.0001055032433563507,
      "loss": 1.007,
      "step": 2260
    },
    {
      "epoch": 1.4249843063402385,
      "grad_norm": 0.6595659852027893,
      "learning_rate": 0.00010508474576271188,
      "loss": 0.9071,
      "step": 2270
    },
    {
      "epoch": 1.4312617702448212,
      "grad_norm": 0.7543580532073975,
      "learning_rate": 0.00010466624816907302,
      "loss": 1.0606,
      "step": 2280
    },
    {
      "epoch": 1.4375392341494035,
      "grad_norm": 0.8915659785270691,
      "learning_rate": 0.00010424775057543418,
      "loss": 1.0913,
      "step": 2290
    },
    {
      "epoch": 1.4438166980539862,
      "grad_norm": 0.6226596832275391,
      "learning_rate": 0.00010382925298179535,
      "loss": 1.0586,
      "step": 2300
    },
    {
      "epoch": 1.4500941619585688,
      "grad_norm": 0.5797991752624512,
      "learning_rate": 0.00010341075538815653,
      "loss": 0.946,
      "step": 2310
    },
    {
      "epoch": 1.4563716258631514,
      "grad_norm": 0.7774348258972168,
      "learning_rate": 0.00010299225779451769,
      "loss": 1.0032,
      "step": 2320
    },
    {
      "epoch": 1.4626490897677338,
      "grad_norm": 0.7795247435569763,
      "learning_rate": 0.00010257376020087886,
      "loss": 1.1007,
      "step": 2330
    },
    {
      "epoch": 1.4689265536723164,
      "grad_norm": 0.7166784405708313,
      "learning_rate": 0.00010215526260724002,
      "loss": 1.067,
      "step": 2340
    },
    {
      "epoch": 1.475204017576899,
      "grad_norm": 0.6890245676040649,
      "learning_rate": 0.00010173676501360119,
      "loss": 0.8749,
      "step": 2350
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 0.7949071526527405,
      "learning_rate": 0.00010131826741996234,
      "loss": 1.0315,
      "step": 2360
    },
    {
      "epoch": 1.487758945386064,
      "grad_norm": 0.8094708919525146,
      "learning_rate": 0.0001008997698263235,
      "loss": 1.0886,
      "step": 2370
    },
    {
      "epoch": 1.4940364092906466,
      "grad_norm": 0.7193349599838257,
      "learning_rate": 0.00010048127223268467,
      "loss": 0.8835,
      "step": 2380
    },
    {
      "epoch": 1.5003138731952292,
      "grad_norm": 0.744175374507904,
      "learning_rate": 0.00010006277463904583,
      "loss": 0.995,
      "step": 2390
    },
    {
      "epoch": 1.5065913370998116,
      "grad_norm": 0.7476231455802917,
      "learning_rate": 9.9644277045407e-05,
      "loss": 0.9914,
      "step": 2400
    },
    {
      "epoch": 1.5128688010043942,
      "grad_norm": 0.7383854985237122,
      "learning_rate": 9.922577945176816e-05,
      "loss": 0.9735,
      "step": 2410
    },
    {
      "epoch": 1.5191462649089766,
      "grad_norm": 0.8346557021141052,
      "learning_rate": 9.880728185812932e-05,
      "loss": 1.0691,
      "step": 2420
    },
    {
      "epoch": 1.5254237288135593,
      "grad_norm": 0.7566072940826416,
      "learning_rate": 9.838878426449049e-05,
      "loss": 0.9959,
      "step": 2430
    },
    {
      "epoch": 1.5317011927181419,
      "grad_norm": 0.6202556490898132,
      "learning_rate": 9.797028667085165e-05,
      "loss": 0.9432,
      "step": 2440
    },
    {
      "epoch": 1.5379786566227245,
      "grad_norm": 0.7331885099411011,
      "learning_rate": 9.75517890772128e-05,
      "loss": 1.0238,
      "step": 2450
    },
    {
      "epoch": 1.544256120527307,
      "grad_norm": 0.7207855582237244,
      "learning_rate": 9.713329148357398e-05,
      "loss": 0.9959,
      "step": 2460
    },
    {
      "epoch": 1.5505335844318895,
      "grad_norm": 0.8413830995559692,
      "learning_rate": 9.671479388993514e-05,
      "loss": 0.9671,
      "step": 2470
    },
    {
      "epoch": 1.556811048336472,
      "grad_norm": 0.6904274225234985,
      "learning_rate": 9.62962962962963e-05,
      "loss": 0.989,
      "step": 2480
    },
    {
      "epoch": 1.5630885122410545,
      "grad_norm": 0.663164496421814,
      "learning_rate": 9.587779870265746e-05,
      "loss": 1.0091,
      "step": 2490
    },
    {
      "epoch": 1.569365976145637,
      "grad_norm": 0.6210168600082397,
      "learning_rate": 9.545930110901863e-05,
      "loss": 0.8921,
      "step": 2500
    },
    {
      "epoch": 1.5756434400502197,
      "grad_norm": 0.6323394179344177,
      "learning_rate": 9.50408035153798e-05,
      "loss": 0.9403,
      "step": 2510
    },
    {
      "epoch": 1.5819209039548023,
      "grad_norm": 0.8274778127670288,
      "learning_rate": 9.462230592174095e-05,
      "loss": 1.0039,
      "step": 2520
    },
    {
      "epoch": 1.588198367859385,
      "grad_norm": 0.6972327828407288,
      "learning_rate": 9.420380832810212e-05,
      "loss": 0.9147,
      "step": 2530
    },
    {
      "epoch": 1.5944758317639673,
      "grad_norm": 0.7473239898681641,
      "learning_rate": 9.378531073446328e-05,
      "loss": 0.9346,
      "step": 2540
    },
    {
      "epoch": 1.60075329566855,
      "grad_norm": 0.6532592177391052,
      "learning_rate": 9.336681314082445e-05,
      "loss": 1.0048,
      "step": 2550
    },
    {
      "epoch": 1.6070307595731324,
      "grad_norm": 0.6692841053009033,
      "learning_rate": 9.294831554718561e-05,
      "loss": 1.0488,
      "step": 2560
    },
    {
      "epoch": 1.613308223477715,
      "grad_norm": 0.7742165923118591,
      "learning_rate": 9.252981795354677e-05,
      "loss": 0.9486,
      "step": 2570
    },
    {
      "epoch": 1.6195856873822976,
      "grad_norm": 0.8424056768417358,
      "learning_rate": 9.211132035990794e-05,
      "loss": 1.0127,
      "step": 2580
    },
    {
      "epoch": 1.6258631512868802,
      "grad_norm": 0.6700634360313416,
      "learning_rate": 9.16928227662691e-05,
      "loss": 0.9782,
      "step": 2590
    },
    {
      "epoch": 1.6321406151914628,
      "grad_norm": 0.7035035490989685,
      "learning_rate": 9.127432517263026e-05,
      "loss": 1.0012,
      "step": 2600
    },
    {
      "epoch": 1.6384180790960452,
      "grad_norm": 0.7649480700492859,
      "learning_rate": 9.085582757899143e-05,
      "loss": 1.042,
      "step": 2610
    },
    {
      "epoch": 1.6446955430006276,
      "grad_norm": 0.6388869881629944,
      "learning_rate": 9.043732998535259e-05,
      "loss": 0.8509,
      "step": 2620
    },
    {
      "epoch": 1.6509730069052102,
      "grad_norm": 0.796817421913147,
      "learning_rate": 9.001883239171375e-05,
      "loss": 0.9922,
      "step": 2630
    },
    {
      "epoch": 1.6572504708097928,
      "grad_norm": 0.6295759081840515,
      "learning_rate": 8.960033479807492e-05,
      "loss": 1.0645,
      "step": 2640
    },
    {
      "epoch": 1.6635279347143754,
      "grad_norm": 0.7451056838035583,
      "learning_rate": 8.918183720443608e-05,
      "loss": 0.9213,
      "step": 2650
    },
    {
      "epoch": 1.669805398618958,
      "grad_norm": 0.6724480390548706,
      "learning_rate": 8.876333961079725e-05,
      "loss": 0.9115,
      "step": 2660
    },
    {
      "epoch": 1.6760828625235404,
      "grad_norm": 0.6724068522453308,
      "learning_rate": 8.83448420171584e-05,
      "loss": 0.921,
      "step": 2670
    },
    {
      "epoch": 1.682360326428123,
      "grad_norm": 0.7878633141517639,
      "learning_rate": 8.792634442351957e-05,
      "loss": 0.9473,
      "step": 2680
    },
    {
      "epoch": 1.6886377903327054,
      "grad_norm": 0.7017163038253784,
      "learning_rate": 8.750784682988074e-05,
      "loss": 0.9705,
      "step": 2690
    },
    {
      "epoch": 1.694915254237288,
      "grad_norm": 0.7194337248802185,
      "learning_rate": 8.708934923624189e-05,
      "loss": 0.9866,
      "step": 2700
    },
    {
      "epoch": 1.7011927181418707,
      "grad_norm": 0.7245289087295532,
      "learning_rate": 8.667085164260306e-05,
      "loss": 1.0458,
      "step": 2710
    },
    {
      "epoch": 1.7074701820464533,
      "grad_norm": 0.7063584327697754,
      "learning_rate": 8.625235404896422e-05,
      "loss": 0.9619,
      "step": 2720
    },
    {
      "epoch": 1.713747645951036,
      "grad_norm": 0.7000411152839661,
      "learning_rate": 8.583385645532539e-05,
      "loss": 0.9813,
      "step": 2730
    },
    {
      "epoch": 1.7200251098556183,
      "grad_norm": 0.7663021683692932,
      "learning_rate": 8.541535886168655e-05,
      "loss": 0.99,
      "step": 2740
    },
    {
      "epoch": 1.726302573760201,
      "grad_norm": 0.776628315448761,
      "learning_rate": 8.499686126804771e-05,
      "loss": 0.9342,
      "step": 2750
    },
    {
      "epoch": 1.7325800376647833,
      "grad_norm": 0.802300751209259,
      "learning_rate": 8.457836367440888e-05,
      "loss": 0.9103,
      "step": 2760
    },
    {
      "epoch": 1.738857501569366,
      "grad_norm": 0.6347243785858154,
      "learning_rate": 8.415986608077004e-05,
      "loss": 1.0132,
      "step": 2770
    },
    {
      "epoch": 1.7451349654739485,
      "grad_norm": 0.6982064247131348,
      "learning_rate": 8.37413684871312e-05,
      "loss": 0.9442,
      "step": 2780
    },
    {
      "epoch": 1.7514124293785311,
      "grad_norm": 0.7480707168579102,
      "learning_rate": 8.332287089349237e-05,
      "loss": 0.9186,
      "step": 2790
    },
    {
      "epoch": 1.7576898932831138,
      "grad_norm": 0.6729371547698975,
      "learning_rate": 8.290437329985353e-05,
      "loss": 1.0003,
      "step": 2800
    },
    {
      "epoch": 1.7639673571876961,
      "grad_norm": 0.7701439261436462,
      "learning_rate": 8.248587570621469e-05,
      "loss": 0.9397,
      "step": 2810
    },
    {
      "epoch": 1.7702448210922788,
      "grad_norm": 0.8375275135040283,
      "learning_rate": 8.206737811257585e-05,
      "loss": 0.8448,
      "step": 2820
    },
    {
      "epoch": 1.7765222849968612,
      "grad_norm": 0.6713117957115173,
      "learning_rate": 8.164888051893702e-05,
      "loss": 0.9339,
      "step": 2830
    },
    {
      "epoch": 1.7827997489014438,
      "grad_norm": 0.7425020933151245,
      "learning_rate": 8.12303829252982e-05,
      "loss": 0.9609,
      "step": 2840
    },
    {
      "epoch": 1.7890772128060264,
      "grad_norm": 0.6967371106147766,
      "learning_rate": 8.081188533165934e-05,
      "loss": 1.0792,
      "step": 2850
    },
    {
      "epoch": 1.795354676710609,
      "grad_norm": 0.7533058524131775,
      "learning_rate": 8.039338773802051e-05,
      "loss": 0.9485,
      "step": 2860
    },
    {
      "epoch": 1.8016321406151916,
      "grad_norm": 0.6905535459518433,
      "learning_rate": 7.997489014438167e-05,
      "loss": 0.9792,
      "step": 2870
    },
    {
      "epoch": 1.807909604519774,
      "grad_norm": 0.8177005648612976,
      "learning_rate": 7.955639255074284e-05,
      "loss": 0.9897,
      "step": 2880
    },
    {
      "epoch": 1.8141870684243564,
      "grad_norm": 0.6851024031639099,
      "learning_rate": 7.9137894957104e-05,
      "loss": 1.0104,
      "step": 2890
    },
    {
      "epoch": 1.820464532328939,
      "grad_norm": 0.8242319822311401,
      "learning_rate": 7.871939736346516e-05,
      "loss": 0.9566,
      "step": 2900
    },
    {
      "epoch": 1.8267419962335216,
      "grad_norm": 0.7438759207725525,
      "learning_rate": 7.830089976982633e-05,
      "loss": 0.9197,
      "step": 2910
    },
    {
      "epoch": 1.8330194601381042,
      "grad_norm": 0.6872438788414001,
      "learning_rate": 7.788240217618749e-05,
      "loss": 0.9863,
      "step": 2920
    },
    {
      "epoch": 1.8392969240426869,
      "grad_norm": 0.6776806712150574,
      "learning_rate": 7.746390458254865e-05,
      "loss": 0.962,
      "step": 2930
    },
    {
      "epoch": 1.8455743879472695,
      "grad_norm": 0.6414613723754883,
      "learning_rate": 7.704540698890982e-05,
      "loss": 1.1004,
      "step": 2940
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 0.7290835976600647,
      "learning_rate": 7.662690939527098e-05,
      "loss": 0.9763,
      "step": 2950
    },
    {
      "epoch": 1.8581293157564343,
      "grad_norm": 0.7501586079597473,
      "learning_rate": 7.620841180163214e-05,
      "loss": 0.9732,
      "step": 2960
    },
    {
      "epoch": 1.8644067796610169,
      "grad_norm": 0.7055151462554932,
      "learning_rate": 7.57899142079933e-05,
      "loss": 1.0116,
      "step": 2970
    },
    {
      "epoch": 1.8706842435655995,
      "grad_norm": 0.8125553131103516,
      "learning_rate": 7.537141661435447e-05,
      "loss": 0.9627,
      "step": 2980
    },
    {
      "epoch": 1.876961707470182,
      "grad_norm": 0.7260874509811401,
      "learning_rate": 7.495291902071564e-05,
      "loss": 0.8459,
      "step": 2990
    },
    {
      "epoch": 1.8832391713747647,
      "grad_norm": 0.6567785143852234,
      "learning_rate": 7.453442142707679e-05,
      "loss": 0.9775,
      "step": 3000
    },
    {
      "epoch": 1.889516635279347,
      "grad_norm": 0.8252705931663513,
      "learning_rate": 7.411592383343796e-05,
      "loss": 1.065,
      "step": 3010
    },
    {
      "epoch": 1.8957940991839297,
      "grad_norm": 0.7368611693382263,
      "learning_rate": 7.369742623979912e-05,
      "loss": 0.9138,
      "step": 3020
    },
    {
      "epoch": 1.902071563088512,
      "grad_norm": 0.6803000569343567,
      "learning_rate": 7.327892864616028e-05,
      "loss": 0.9336,
      "step": 3030
    },
    {
      "epoch": 1.9083490269930947,
      "grad_norm": 0.7635763883590698,
      "learning_rate": 7.286043105252145e-05,
      "loss": 0.9738,
      "step": 3040
    },
    {
      "epoch": 1.9146264908976773,
      "grad_norm": 0.7305115461349487,
      "learning_rate": 7.244193345888261e-05,
      "loss": 0.8895,
      "step": 3050
    },
    {
      "epoch": 1.92090395480226,
      "grad_norm": 0.5934025645256042,
      "learning_rate": 7.202343586524378e-05,
      "loss": 0.9573,
      "step": 3060
    },
    {
      "epoch": 1.9271814187068426,
      "grad_norm": 0.7836874723434448,
      "learning_rate": 7.160493827160494e-05,
      "loss": 0.9461,
      "step": 3070
    },
    {
      "epoch": 1.933458882611425,
      "grad_norm": 0.7988609671592712,
      "learning_rate": 7.11864406779661e-05,
      "loss": 0.9549,
      "step": 3080
    },
    {
      "epoch": 1.9397363465160076,
      "grad_norm": 0.7216009497642517,
      "learning_rate": 7.076794308432727e-05,
      "loss": 0.9431,
      "step": 3090
    },
    {
      "epoch": 1.94601381042059,
      "grad_norm": 0.7198189496994019,
      "learning_rate": 7.034944549068843e-05,
      "loss": 0.8438,
      "step": 3100
    },
    {
      "epoch": 1.9522912743251726,
      "grad_norm": 0.7556623220443726,
      "learning_rate": 6.993094789704959e-05,
      "loss": 0.9724,
      "step": 3110
    },
    {
      "epoch": 1.9585687382297552,
      "grad_norm": 0.724575400352478,
      "learning_rate": 6.951245030341076e-05,
      "loss": 0.9148,
      "step": 3120
    },
    {
      "epoch": 1.9648462021343378,
      "grad_norm": 0.646049976348877,
      "learning_rate": 6.909395270977192e-05,
      "loss": 0.8773,
      "step": 3130
    },
    {
      "epoch": 1.9711236660389204,
      "grad_norm": 0.749110221862793,
      "learning_rate": 6.867545511613308e-05,
      "loss": 1.0309,
      "step": 3140
    },
    {
      "epoch": 1.9774011299435028,
      "grad_norm": 0.7617254257202148,
      "learning_rate": 6.825695752249424e-05,
      "loss": 1.0986,
      "step": 3150
    },
    {
      "epoch": 1.9836785938480854,
      "grad_norm": 0.6587416529655457,
      "learning_rate": 6.783845992885541e-05,
      "loss": 1.0085,
      "step": 3160
    },
    {
      "epoch": 1.9899560577526678,
      "grad_norm": 0.7511833310127258,
      "learning_rate": 6.741996233521659e-05,
      "loss": 0.9077,
      "step": 3170
    },
    {
      "epoch": 1.9962335216572504,
      "grad_norm": 0.8248170018196106,
      "learning_rate": 6.700146474157773e-05,
      "loss": 0.9994,
      "step": 3180
    },
    {
      "epoch": 2.002510985561833,
      "grad_norm": 0.6467968225479126,
      "learning_rate": 6.65829671479389e-05,
      "loss": 0.9974,
      "step": 3190
    },
    {
      "epoch": 2.0087884494664157,
      "grad_norm": 0.723430335521698,
      "learning_rate": 6.616446955430006e-05,
      "loss": 0.8278,
      "step": 3200
    },
    {
      "epoch": 2.0150659133709983,
      "grad_norm": 0.7244954109191895,
      "learning_rate": 6.574597196066124e-05,
      "loss": 0.9853,
      "step": 3210
    },
    {
      "epoch": 2.0213433772755804,
      "grad_norm": 0.7277804613113403,
      "learning_rate": 6.53274743670224e-05,
      "loss": 0.8879,
      "step": 3220
    },
    {
      "epoch": 2.027620841180163,
      "grad_norm": 0.7696599960327148,
      "learning_rate": 6.490897677338355e-05,
      "loss": 0.8977,
      "step": 3230
    },
    {
      "epoch": 2.0338983050847457,
      "grad_norm": 0.6953431963920593,
      "learning_rate": 6.449047917974473e-05,
      "loss": 1.0187,
      "step": 3240
    },
    {
      "epoch": 2.0401757689893283,
      "grad_norm": 0.723072350025177,
      "learning_rate": 6.407198158610587e-05,
      "loss": 0.8652,
      "step": 3250
    },
    {
      "epoch": 2.046453232893911,
      "grad_norm": 0.7716438174247742,
      "learning_rate": 6.365348399246704e-05,
      "loss": 0.8767,
      "step": 3260
    },
    {
      "epoch": 2.0527306967984935,
      "grad_norm": 0.7233049869537354,
      "learning_rate": 6.323498639882822e-05,
      "loss": 0.8756,
      "step": 3270
    },
    {
      "epoch": 2.059008160703076,
      "grad_norm": 0.7348067164421082,
      "learning_rate": 6.281648880518938e-05,
      "loss": 0.9156,
      "step": 3280
    },
    {
      "epoch": 2.0652856246076583,
      "grad_norm": 0.7880986332893372,
      "learning_rate": 6.239799121155053e-05,
      "loss": 0.9339,
      "step": 3290
    },
    {
      "epoch": 2.071563088512241,
      "grad_norm": 0.7089604139328003,
      "learning_rate": 6.197949361791169e-05,
      "loss": 0.9086,
      "step": 3300
    },
    {
      "epoch": 2.0778405524168235,
      "grad_norm": 0.7018082141876221,
      "learning_rate": 6.156099602427287e-05,
      "loss": 0.9669,
      "step": 3310
    },
    {
      "epoch": 2.084118016321406,
      "grad_norm": 0.6838268637657166,
      "learning_rate": 6.114249843063404e-05,
      "loss": 0.883,
      "step": 3320
    },
    {
      "epoch": 2.0903954802259888,
      "grad_norm": 0.7405213117599487,
      "learning_rate": 6.072400083699519e-05,
      "loss": 0.8442,
      "step": 3330
    },
    {
      "epoch": 2.0966729441305714,
      "grad_norm": 0.7965055704116821,
      "learning_rate": 6.0305503243356356e-05,
      "loss": 0.92,
      "step": 3340
    },
    {
      "epoch": 2.102950408035154,
      "grad_norm": 0.7786940336227417,
      "learning_rate": 5.988700564971752e-05,
      "loss": 0.9858,
      "step": 3350
    },
    {
      "epoch": 2.109227871939736,
      "grad_norm": 0.7571512460708618,
      "learning_rate": 5.9468508056078674e-05,
      "loss": 0.8756,
      "step": 3360
    },
    {
      "epoch": 2.1155053358443188,
      "grad_norm": 0.7993537783622742,
      "learning_rate": 5.905001046243984e-05,
      "loss": 0.9249,
      "step": 3370
    },
    {
      "epoch": 2.1217827997489014,
      "grad_norm": 0.85566645860672,
      "learning_rate": 5.863151286880101e-05,
      "loss": 0.9385,
      "step": 3380
    },
    {
      "epoch": 2.128060263653484,
      "grad_norm": 0.7722708582878113,
      "learning_rate": 5.821301527516218e-05,
      "loss": 0.99,
      "step": 3390
    },
    {
      "epoch": 2.1343377275580666,
      "grad_norm": 0.7695317268371582,
      "learning_rate": 5.779451768152333e-05,
      "loss": 0.897,
      "step": 3400
    },
    {
      "epoch": 2.1406151914626492,
      "grad_norm": 0.850713312625885,
      "learning_rate": 5.7376020087884495e-05,
      "loss": 0.9409,
      "step": 3410
    },
    {
      "epoch": 2.146892655367232,
      "grad_norm": 0.709577739238739,
      "learning_rate": 5.695752249424566e-05,
      "loss": 1.009,
      "step": 3420
    },
    {
      "epoch": 2.153170119271814,
      "grad_norm": 0.7047581076622009,
      "learning_rate": 5.653902490060683e-05,
      "loss": 0.9893,
      "step": 3430
    },
    {
      "epoch": 2.1594475831763966,
      "grad_norm": 0.664852499961853,
      "learning_rate": 5.6120527306967986e-05,
      "loss": 0.987,
      "step": 3440
    },
    {
      "epoch": 2.1657250470809792,
      "grad_norm": 1.0020644664764404,
      "learning_rate": 5.570202971332915e-05,
      "loss": 0.9414,
      "step": 3450
    },
    {
      "epoch": 2.172002510985562,
      "grad_norm": 0.8719198703765869,
      "learning_rate": 5.528353211969032e-05,
      "loss": 1.0383,
      "step": 3460
    },
    {
      "epoch": 2.1782799748901445,
      "grad_norm": 0.849536120891571,
      "learning_rate": 5.4865034526051476e-05,
      "loss": 0.8978,
      "step": 3470
    },
    {
      "epoch": 2.184557438794727,
      "grad_norm": 0.8062014579772949,
      "learning_rate": 5.444653693241264e-05,
      "loss": 0.9465,
      "step": 3480
    },
    {
      "epoch": 2.1908349026993097,
      "grad_norm": 0.6975785493850708,
      "learning_rate": 5.402803933877381e-05,
      "loss": 0.9547,
      "step": 3490
    },
    {
      "epoch": 2.197112366603892,
      "grad_norm": 0.7735097408294678,
      "learning_rate": 5.360954174513497e-05,
      "loss": 0.9695,
      "step": 3500
    },
    {
      "epoch": 2.2033898305084745,
      "grad_norm": 0.6829891800880432,
      "learning_rate": 5.3191044151496125e-05,
      "loss": 0.962,
      "step": 3510
    },
    {
      "epoch": 2.209667294413057,
      "grad_norm": 0.6647287607192993,
      "learning_rate": 5.27725465578573e-05,
      "loss": 0.8944,
      "step": 3520
    },
    {
      "epoch": 2.2159447583176397,
      "grad_norm": 0.680009126663208,
      "learning_rate": 5.235404896421846e-05,
      "loss": 0.867,
      "step": 3530
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.7038944959640503,
      "learning_rate": 5.193555137057963e-05,
      "loss": 1.0771,
      "step": 3540
    },
    {
      "epoch": 2.228499686126805,
      "grad_norm": 0.6590725183486938,
      "learning_rate": 5.151705377694078e-05,
      "loss": 0.9229,
      "step": 3550
    },
    {
      "epoch": 2.2347771500313875,
      "grad_norm": 0.7869805693626404,
      "learning_rate": 5.109855618330195e-05,
      "loss": 0.9407,
      "step": 3560
    },
    {
      "epoch": 2.2410546139359697,
      "grad_norm": 0.7077551484107971,
      "learning_rate": 5.068005858966311e-05,
      "loss": 0.8879,
      "step": 3570
    },
    {
      "epoch": 2.2473320778405523,
      "grad_norm": 0.8932343125343323,
      "learning_rate": 5.026156099602427e-05,
      "loss": 0.8425,
      "step": 3580
    },
    {
      "epoch": 2.253609541745135,
      "grad_norm": 0.8650214076042175,
      "learning_rate": 4.984306340238544e-05,
      "loss": 0.9657,
      "step": 3590
    },
    {
      "epoch": 2.2598870056497176,
      "grad_norm": 0.8256680965423584,
      "learning_rate": 4.94245658087466e-05,
      "loss": 0.9318,
      "step": 3600
    },
    {
      "epoch": 2.2661644695543,
      "grad_norm": 0.8826923370361328,
      "learning_rate": 4.900606821510776e-05,
      "loss": 0.9432,
      "step": 3610
    },
    {
      "epoch": 2.272441933458883,
      "grad_norm": 0.7974663376808167,
      "learning_rate": 4.8587570621468934e-05,
      "loss": 0.8775,
      "step": 3620
    },
    {
      "epoch": 2.2787193973634654,
      "grad_norm": 0.8559159636497498,
      "learning_rate": 4.816907302783009e-05,
      "loss": 0.9382,
      "step": 3630
    },
    {
      "epoch": 2.2849968612680476,
      "grad_norm": 0.6735168695449829,
      "learning_rate": 4.775057543419126e-05,
      "loss": 0.9251,
      "step": 3640
    },
    {
      "epoch": 2.29127432517263,
      "grad_norm": 0.7733509540557861,
      "learning_rate": 4.733207784055242e-05,
      "loss": 0.9234,
      "step": 3650
    },
    {
      "epoch": 2.297551789077213,
      "grad_norm": 0.749370813369751,
      "learning_rate": 4.691358024691358e-05,
      "loss": 0.9705,
      "step": 3660
    },
    {
      "epoch": 2.3038292529817954,
      "grad_norm": 0.7883768677711487,
      "learning_rate": 4.649508265327475e-05,
      "loss": 0.9794,
      "step": 3670
    },
    {
      "epoch": 2.310106716886378,
      "grad_norm": 0.6738448143005371,
      "learning_rate": 4.607658505963591e-05,
      "loss": 0.8749,
      "step": 3680
    },
    {
      "epoch": 2.3163841807909606,
      "grad_norm": 0.7514053583145142,
      "learning_rate": 4.5658087465997074e-05,
      "loss": 0.8729,
      "step": 3690
    },
    {
      "epoch": 2.322661644695543,
      "grad_norm": 0.822688102722168,
      "learning_rate": 4.523958987235823e-05,
      "loss": 0.8827,
      "step": 3700
    },
    {
      "epoch": 2.3289391086001254,
      "grad_norm": 0.7176437377929688,
      "learning_rate": 4.48210922787194e-05,
      "loss": 1.0038,
      "step": 3710
    },
    {
      "epoch": 2.335216572504708,
      "grad_norm": 0.8740195631980896,
      "learning_rate": 4.4402594685080564e-05,
      "loss": 0.9561,
      "step": 3720
    },
    {
      "epoch": 2.3414940364092907,
      "grad_norm": 0.7539896368980408,
      "learning_rate": 4.398409709144173e-05,
      "loss": 0.9733,
      "step": 3730
    },
    {
      "epoch": 2.3477715003138733,
      "grad_norm": 0.6971808671951294,
      "learning_rate": 4.356559949780289e-05,
      "loss": 0.9464,
      "step": 3740
    },
    {
      "epoch": 2.354048964218456,
      "grad_norm": 0.7001827359199524,
      "learning_rate": 4.3147101904164054e-05,
      "loss": 0.8816,
      "step": 3750
    },
    {
      "epoch": 2.360326428123038,
      "grad_norm": 0.6344919800758362,
      "learning_rate": 4.272860431052522e-05,
      "loss": 0.8613,
      "step": 3760
    },
    {
      "epoch": 2.3666038920276207,
      "grad_norm": 0.7363542914390564,
      "learning_rate": 4.231010671688638e-05,
      "loss": 0.8834,
      "step": 3770
    },
    {
      "epoch": 2.3728813559322033,
      "grad_norm": 0.659002423286438,
      "learning_rate": 4.1891609123247544e-05,
      "loss": 0.9753,
      "step": 3780
    },
    {
      "epoch": 2.379158819836786,
      "grad_norm": 0.753095269203186,
      "learning_rate": 4.14731115296087e-05,
      "loss": 0.8872,
      "step": 3790
    },
    {
      "epoch": 2.3854362837413685,
      "grad_norm": 0.7748133540153503,
      "learning_rate": 4.105461393596987e-05,
      "loss": 0.9466,
      "step": 3800
    },
    {
      "epoch": 2.391713747645951,
      "grad_norm": 0.7649117708206177,
      "learning_rate": 4.0636116342331035e-05,
      "loss": 0.9914,
      "step": 3810
    },
    {
      "epoch": 2.3979912115505337,
      "grad_norm": 0.826468288898468,
      "learning_rate": 4.02176187486922e-05,
      "loss": 0.9215,
      "step": 3820
    },
    {
      "epoch": 2.404268675455116,
      "grad_norm": 0.7038507461547852,
      "learning_rate": 3.979912115505336e-05,
      "loss": 0.9048,
      "step": 3830
    },
    {
      "epoch": 2.4105461393596985,
      "grad_norm": 0.7854218482971191,
      "learning_rate": 3.9380623561414525e-05,
      "loss": 0.9245,
      "step": 3840
    },
    {
      "epoch": 2.416823603264281,
      "grad_norm": 0.7577252388000488,
      "learning_rate": 3.8962125967775684e-05,
      "loss": 0.9152,
      "step": 3850
    },
    {
      "epoch": 2.4231010671688638,
      "grad_norm": 0.6871301531791687,
      "learning_rate": 3.8543628374136856e-05,
      "loss": 0.9657,
      "step": 3860
    },
    {
      "epoch": 2.4293785310734464,
      "grad_norm": 0.7999315857887268,
      "learning_rate": 3.8125130780498015e-05,
      "loss": 0.9039,
      "step": 3870
    },
    {
      "epoch": 2.435655994978029,
      "grad_norm": 0.6344884037971497,
      "learning_rate": 3.7706633186859174e-05,
      "loss": 0.8554,
      "step": 3880
    },
    {
      "epoch": 2.4419334588826116,
      "grad_norm": 0.8203125,
      "learning_rate": 3.728813559322034e-05,
      "loss": 0.9072,
      "step": 3890
    },
    {
      "epoch": 2.4482109227871938,
      "grad_norm": 0.8089978694915771,
      "learning_rate": 3.6869637999581506e-05,
      "loss": 0.8683,
      "step": 3900
    },
    {
      "epoch": 2.4544883866917764,
      "grad_norm": 0.6684524416923523,
      "learning_rate": 3.645114040594267e-05,
      "loss": 0.9435,
      "step": 3910
    },
    {
      "epoch": 2.460765850596359,
      "grad_norm": 0.9804052114486694,
      "learning_rate": 3.603264281230383e-05,
      "loss": 0.9603,
      "step": 3920
    },
    {
      "epoch": 2.4670433145009416,
      "grad_norm": 0.7532960176467896,
      "learning_rate": 3.5614145218664996e-05,
      "loss": 0.8927,
      "step": 3930
    },
    {
      "epoch": 2.4733207784055242,
      "grad_norm": 0.8518887758255005,
      "learning_rate": 3.5195647625026155e-05,
      "loss": 0.8217,
      "step": 3940
    },
    {
      "epoch": 2.479598242310107,
      "grad_norm": 0.7100833058357239,
      "learning_rate": 3.477715003138732e-05,
      "loss": 0.9951,
      "step": 3950
    },
    {
      "epoch": 2.4858757062146895,
      "grad_norm": 0.6787739992141724,
      "learning_rate": 3.4358652437748486e-05,
      "loss": 0.8792,
      "step": 3960
    },
    {
      "epoch": 2.4921531701192716,
      "grad_norm": 0.6890696883201599,
      "learning_rate": 3.394015484410965e-05,
      "loss": 0.9858,
      "step": 3970
    },
    {
      "epoch": 2.4984306340238542,
      "grad_norm": 0.7172227501869202,
      "learning_rate": 3.352165725047081e-05,
      "loss": 0.8442,
      "step": 3980
    },
    {
      "epoch": 2.504708097928437,
      "grad_norm": 0.7425734996795654,
      "learning_rate": 3.310315965683197e-05,
      "loss": 0.97,
      "step": 3990
    },
    {
      "epoch": 2.5109855618330195,
      "grad_norm": 0.7293495535850525,
      "learning_rate": 3.268466206319314e-05,
      "loss": 0.9658,
      "step": 4000
    },
    {
      "epoch": 2.517263025737602,
      "grad_norm": 0.7423468828201294,
      "learning_rate": 3.22661644695543e-05,
      "loss": 0.9156,
      "step": 4010
    },
    {
      "epoch": 2.5235404896421847,
      "grad_norm": 0.8508827090263367,
      "learning_rate": 3.184766687591547e-05,
      "loss": 0.9871,
      "step": 4020
    },
    {
      "epoch": 2.5298179535467673,
      "grad_norm": 0.7457152009010315,
      "learning_rate": 3.1429169282276626e-05,
      "loss": 0.891,
      "step": 4030
    },
    {
      "epoch": 2.5360954174513495,
      "grad_norm": 0.8813952207565308,
      "learning_rate": 3.101067168863779e-05,
      "loss": 0.9844,
      "step": 4040
    },
    {
      "epoch": 2.542372881355932,
      "grad_norm": 0.8568426966667175,
      "learning_rate": 3.059217409499896e-05,
      "loss": 0.9011,
      "step": 4050
    },
    {
      "epoch": 2.5486503452605147,
      "grad_norm": 0.7734302282333374,
      "learning_rate": 3.017367650136012e-05,
      "loss": 0.9592,
      "step": 4060
    },
    {
      "epoch": 2.5549278091650973,
      "grad_norm": 0.7530774474143982,
      "learning_rate": 2.975517890772128e-05,
      "loss": 0.8733,
      "step": 4070
    },
    {
      "epoch": 2.56120527306968,
      "grad_norm": 0.6179295778274536,
      "learning_rate": 2.9336681314082447e-05,
      "loss": 0.8255,
      "step": 4080
    },
    {
      "epoch": 2.5674827369742625,
      "grad_norm": 0.862443208694458,
      "learning_rate": 2.891818372044361e-05,
      "loss": 1.0808,
      "step": 4090
    },
    {
      "epoch": 2.573760200878845,
      "grad_norm": 0.7164528965950012,
      "learning_rate": 2.849968612680477e-05,
      "loss": 0.9281,
      "step": 4100
    },
    {
      "epoch": 2.5800376647834273,
      "grad_norm": 0.7631130814552307,
      "learning_rate": 2.8081188533165938e-05,
      "loss": 0.9364,
      "step": 4110
    },
    {
      "epoch": 2.58631512868801,
      "grad_norm": 0.686453640460968,
      "learning_rate": 2.7662690939527096e-05,
      "loss": 0.8325,
      "step": 4120
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 0.8067458271980286,
      "learning_rate": 2.7244193345888262e-05,
      "loss": 0.8958,
      "step": 4130
    },
    {
      "epoch": 2.598870056497175,
      "grad_norm": 0.8332718014717102,
      "learning_rate": 2.6825695752249424e-05,
      "loss": 0.9869,
      "step": 4140
    },
    {
      "epoch": 2.605147520401758,
      "grad_norm": 0.8236085176467896,
      "learning_rate": 2.640719815861059e-05,
      "loss": 0.9405,
      "step": 4150
    },
    {
      "epoch": 2.6114249843063404,
      "grad_norm": 0.7838770151138306,
      "learning_rate": 2.5988700564971752e-05,
      "loss": 0.9577,
      "step": 4160
    },
    {
      "epoch": 2.617702448210923,
      "grad_norm": 0.6905799508094788,
      "learning_rate": 2.5570202971332918e-05,
      "loss": 0.8814,
      "step": 4170
    },
    {
      "epoch": 2.623979912115505,
      "grad_norm": 0.731666624546051,
      "learning_rate": 2.515170537769408e-05,
      "loss": 1.0066,
      "step": 4180
    },
    {
      "epoch": 2.630257376020088,
      "grad_norm": 0.7857332229614258,
      "learning_rate": 2.4733207784055243e-05,
      "loss": 0.959,
      "step": 4190
    },
    {
      "epoch": 2.6365348399246704,
      "grad_norm": 0.8998384475708008,
      "learning_rate": 2.4314710190416405e-05,
      "loss": 0.9778,
      "step": 4200
    },
    {
      "epoch": 2.642812303829253,
      "grad_norm": 0.8380339741706848,
      "learning_rate": 2.389621259677757e-05,
      "loss": 0.9311,
      "step": 4210
    },
    {
      "epoch": 2.6490897677338356,
      "grad_norm": 0.7865540981292725,
      "learning_rate": 2.3477715003138733e-05,
      "loss": 0.9099,
      "step": 4220
    },
    {
      "epoch": 2.655367231638418,
      "grad_norm": 0.7019716501235962,
      "learning_rate": 2.30592174094999e-05,
      "loss": 0.9357,
      "step": 4230
    },
    {
      "epoch": 2.661644695543001,
      "grad_norm": 0.7495397925376892,
      "learning_rate": 2.264071981586106e-05,
      "loss": 0.915,
      "step": 4240
    },
    {
      "epoch": 2.667922159447583,
      "grad_norm": 0.7091226577758789,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.8902,
      "step": 4250
    },
    {
      "epoch": 2.6741996233521657,
      "grad_norm": 0.785758912563324,
      "learning_rate": 2.1803724628583386e-05,
      "loss": 0.8942,
      "step": 4260
    },
    {
      "epoch": 2.6804770872567483,
      "grad_norm": 0.825289785861969,
      "learning_rate": 2.1385227034944548e-05,
      "loss": 0.9289,
      "step": 4270
    },
    {
      "epoch": 2.686754551161331,
      "grad_norm": 0.8194184899330139,
      "learning_rate": 2.0966729441305714e-05,
      "loss": 0.9651,
      "step": 4280
    },
    {
      "epoch": 2.6930320150659135,
      "grad_norm": 0.8228126764297485,
      "learning_rate": 2.0548231847666876e-05,
      "loss": 0.9292,
      "step": 4290
    },
    {
      "epoch": 2.6993094789704957,
      "grad_norm": 0.8039390444755554,
      "learning_rate": 2.012973425402804e-05,
      "loss": 0.9337,
      "step": 4300
    },
    {
      "epoch": 2.7055869428750787,
      "grad_norm": 0.8188043236732483,
      "learning_rate": 1.9711236660389204e-05,
      "loss": 1.0101,
      "step": 4310
    },
    {
      "epoch": 2.711864406779661,
      "grad_norm": 0.6641451716423035,
      "learning_rate": 1.9292739066750366e-05,
      "loss": 0.9236,
      "step": 4320
    },
    {
      "epoch": 2.7181418706842435,
      "grad_norm": 0.7890886664390564,
      "learning_rate": 1.8874241473111532e-05,
      "loss": 0.937,
      "step": 4330
    },
    {
      "epoch": 2.724419334588826,
      "grad_norm": 0.8769708871841431,
      "learning_rate": 1.8455743879472694e-05,
      "loss": 1.0055,
      "step": 4340
    },
    {
      "epoch": 2.7306967984934087,
      "grad_norm": 0.8037331700325012,
      "learning_rate": 1.803724628583386e-05,
      "loss": 0.8082,
      "step": 4350
    },
    {
      "epoch": 2.7369742623979914,
      "grad_norm": 0.7253694534301758,
      "learning_rate": 1.761874869219502e-05,
      "loss": 0.9893,
      "step": 4360
    },
    {
      "epoch": 2.7432517263025735,
      "grad_norm": 0.7510047554969788,
      "learning_rate": 1.7200251098556184e-05,
      "loss": 0.8783,
      "step": 4370
    },
    {
      "epoch": 2.7495291902071566,
      "grad_norm": 0.8462818264961243,
      "learning_rate": 1.6781753504917347e-05,
      "loss": 0.9513,
      "step": 4380
    },
    {
      "epoch": 2.7558066541117388,
      "grad_norm": 0.7766866683959961,
      "learning_rate": 1.636325591127851e-05,
      "loss": 0.8721,
      "step": 4390
    },
    {
      "epoch": 2.7620841180163214,
      "grad_norm": 0.714205801486969,
      "learning_rate": 1.5944758317639675e-05,
      "loss": 0.9259,
      "step": 4400
    },
    {
      "epoch": 2.768361581920904,
      "grad_norm": 0.7380563616752625,
      "learning_rate": 1.5526260724000837e-05,
      "loss": 1.0379,
      "step": 4410
    },
    {
      "epoch": 2.7746390458254866,
      "grad_norm": 0.8354098796844482,
      "learning_rate": 1.5107763130362001e-05,
      "loss": 0.9266,
      "step": 4420
    },
    {
      "epoch": 2.780916509730069,
      "grad_norm": 0.7208314538002014,
      "learning_rate": 1.4689265536723165e-05,
      "loss": 0.8787,
      "step": 4430
    },
    {
      "epoch": 2.7871939736346514,
      "grad_norm": 0.7626462578773499,
      "learning_rate": 1.4270767943084329e-05,
      "loss": 0.9697,
      "step": 4440
    },
    {
      "epoch": 2.793471437539234,
      "grad_norm": 0.7305912375450134,
      "learning_rate": 1.3852270349445493e-05,
      "loss": 0.8969,
      "step": 4450
    },
    {
      "epoch": 2.7997489014438166,
      "grad_norm": 0.7060752511024475,
      "learning_rate": 1.3433772755806654e-05,
      "loss": 0.9787,
      "step": 4460
    },
    {
      "epoch": 2.806026365348399,
      "grad_norm": 0.7729554772377014,
      "learning_rate": 1.3015275162167818e-05,
      "loss": 0.9879,
      "step": 4470
    },
    {
      "epoch": 2.812303829252982,
      "grad_norm": 0.6980060338973999,
      "learning_rate": 1.259677756852898e-05,
      "loss": 0.9074,
      "step": 4480
    },
    {
      "epoch": 2.8185812931575644,
      "grad_norm": 0.7587530016899109,
      "learning_rate": 1.2178279974890144e-05,
      "loss": 0.8549,
      "step": 4490
    },
    {
      "epoch": 2.824858757062147,
      "grad_norm": 0.6958063244819641,
      "learning_rate": 1.1759782381251308e-05,
      "loss": 0.9291,
      "step": 4500
    },
    {
      "epoch": 2.8311362209667292,
      "grad_norm": 0.8952913284301758,
      "learning_rate": 1.1341284787612472e-05,
      "loss": 0.9633,
      "step": 4510
    },
    {
      "epoch": 2.837413684871312,
      "grad_norm": 1.0088104009628296,
      "learning_rate": 1.0922787193973636e-05,
      "loss": 0.9737,
      "step": 4520
    },
    {
      "epoch": 2.8436911487758945,
      "grad_norm": 0.9250949025154114,
      "learning_rate": 1.05042896003348e-05,
      "loss": 0.9296,
      "step": 4530
    },
    {
      "epoch": 2.849968612680477,
      "grad_norm": 0.7983298301696777,
      "learning_rate": 1.008579200669596e-05,
      "loss": 0.991,
      "step": 4540
    },
    {
      "epoch": 2.8562460765850597,
      "grad_norm": 0.8255513310432434,
      "learning_rate": 9.667294413057124e-06,
      "loss": 0.9379,
      "step": 4550
    },
    {
      "epoch": 2.8625235404896423,
      "grad_norm": 0.7975689172744751,
      "learning_rate": 9.248796819418288e-06,
      "loss": 0.9459,
      "step": 4560
    },
    {
      "epoch": 2.868801004394225,
      "grad_norm": 0.8360256552696228,
      "learning_rate": 8.830299225779452e-06,
      "loss": 0.9186,
      "step": 4570
    },
    {
      "epoch": 2.875078468298807,
      "grad_norm": 0.8251099586486816,
      "learning_rate": 8.411801632140616e-06,
      "loss": 0.9317,
      "step": 4580
    },
    {
      "epoch": 2.8813559322033897,
      "grad_norm": 0.8217182755470276,
      "learning_rate": 7.99330403850178e-06,
      "loss": 0.9223,
      "step": 4590
    },
    {
      "epoch": 2.8876333961079723,
      "grad_norm": 0.7815667390823364,
      "learning_rate": 7.574806444862942e-06,
      "loss": 0.8231,
      "step": 4600
    },
    {
      "epoch": 2.893910860012555,
      "grad_norm": 0.8546732664108276,
      "learning_rate": 7.156308851224106e-06,
      "loss": 0.9776,
      "step": 4610
    },
    {
      "epoch": 2.9001883239171375,
      "grad_norm": 0.6987045407295227,
      "learning_rate": 6.737811257585269e-06,
      "loss": 0.9048,
      "step": 4620
    },
    {
      "epoch": 2.90646578782172,
      "grad_norm": 0.8157296776771545,
      "learning_rate": 6.319313663946433e-06,
      "loss": 0.8923,
      "step": 4630
    },
    {
      "epoch": 2.9127432517263028,
      "grad_norm": 0.7278445959091187,
      "learning_rate": 5.900816070307596e-06,
      "loss": 0.9026,
      "step": 4640
    },
    {
      "epoch": 2.919020715630885,
      "grad_norm": 0.8198418617248535,
      "learning_rate": 5.482318476668759e-06,
      "loss": 0.893,
      "step": 4650
    },
    {
      "epoch": 2.9252981795354676,
      "grad_norm": 0.8289986252784729,
      "learning_rate": 5.063820883029923e-06,
      "loss": 0.8735,
      "step": 4660
    },
    {
      "epoch": 2.93157564344005,
      "grad_norm": 0.8788705468177795,
      "learning_rate": 4.6453232893910864e-06,
      "loss": 0.9104,
      "step": 4670
    },
    {
      "epoch": 2.937853107344633,
      "grad_norm": 0.7620707154273987,
      "learning_rate": 4.22682569575225e-06,
      "loss": 0.9491,
      "step": 4680
    },
    {
      "epoch": 2.9441305712492154,
      "grad_norm": 0.7772890329360962,
      "learning_rate": 3.8083281021134127e-06,
      "loss": 0.9085,
      "step": 4690
    },
    {
      "epoch": 2.950408035153798,
      "grad_norm": 0.871261715888977,
      "learning_rate": 3.3898305084745763e-06,
      "loss": 0.8776,
      "step": 4700
    },
    {
      "epoch": 2.9566854990583806,
      "grad_norm": 0.8796091079711914,
      "learning_rate": 2.97133291483574e-06,
      "loss": 0.9751,
      "step": 4710
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 0.8055641651153564,
      "learning_rate": 2.5528353211969035e-06,
      "loss": 0.9293,
      "step": 4720
    },
    {
      "epoch": 2.9692404268675454,
      "grad_norm": 1.007045030593872,
      "learning_rate": 2.1343377275580666e-06,
      "loss": 0.9249,
      "step": 4730
    },
    {
      "epoch": 2.975517890772128,
      "grad_norm": 0.7754725813865662,
      "learning_rate": 1.71584013391923e-06,
      "loss": 0.9373,
      "step": 4740
    },
    {
      "epoch": 2.9817953546767106,
      "grad_norm": 0.7523244619369507,
      "learning_rate": 1.2973425402803935e-06,
      "loss": 0.8371,
      "step": 4750
    },
    {
      "epoch": 2.9880728185812933,
      "grad_norm": 0.7346819043159485,
      "learning_rate": 8.788449466415568e-07,
      "loss": 0.8451,
      "step": 4760
    },
    {
      "epoch": 2.994350282485876,
      "grad_norm": 0.8092731833457947,
      "learning_rate": 4.6034735300272024e-07,
      "loss": 0.8596,
      "step": 4770
    }
  ],
  "logging_steps": 10,
  "max_steps": 4779,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.3159702637235405e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
