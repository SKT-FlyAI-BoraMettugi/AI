{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 3186,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006277463904582548,
      "grad_norm": 0.932168185710907,
      "learning_rate": 0.00019966520192508892,
      "loss": 2.9256,
      "step": 10
    },
    {
      "epoch": 0.012554927809165096,
      "grad_norm": 0.7231044173240662,
      "learning_rate": 0.0001992467043314501,
      "loss": 1.9155,
      "step": 20
    },
    {
      "epoch": 0.018832391713747645,
      "grad_norm": 0.7482074499130249,
      "learning_rate": 0.00019882820673781127,
      "loss": 1.7249,
      "step": 30
    },
    {
      "epoch": 0.025109855618330193,
      "grad_norm": 0.6125890612602234,
      "learning_rate": 0.00019840970914417242,
      "loss": 1.3715,
      "step": 40
    },
    {
      "epoch": 0.031387319522912745,
      "grad_norm": 0.5538865923881531,
      "learning_rate": 0.00019799121155053358,
      "loss": 1.4044,
      "step": 50
    },
    {
      "epoch": 0.03766478342749529,
      "grad_norm": 0.49625977873802185,
      "learning_rate": 0.00019757271395689477,
      "loss": 1.3989,
      "step": 60
    },
    {
      "epoch": 0.04394224733207784,
      "grad_norm": 0.4827761650085449,
      "learning_rate": 0.00019715421636325593,
      "loss": 1.4428,
      "step": 70
    },
    {
      "epoch": 0.050219711236660386,
      "grad_norm": 0.4940015971660614,
      "learning_rate": 0.0001967357187696171,
      "loss": 1.3618,
      "step": 80
    },
    {
      "epoch": 0.05649717514124294,
      "grad_norm": 0.5945996642112732,
      "learning_rate": 0.00019631722117597825,
      "loss": 1.363,
      "step": 90
    },
    {
      "epoch": 0.06277463904582549,
      "grad_norm": 0.553776741027832,
      "learning_rate": 0.0001958987235823394,
      "loss": 1.4174,
      "step": 100
    },
    {
      "epoch": 0.06905210295040803,
      "grad_norm": 0.5640967488288879,
      "learning_rate": 0.00019548022598870056,
      "loss": 1.3061,
      "step": 110
    },
    {
      "epoch": 0.07532956685499058,
      "grad_norm": 0.5838500261306763,
      "learning_rate": 0.00019506172839506175,
      "loss": 1.2411,
      "step": 120
    },
    {
      "epoch": 0.08160703075957314,
      "grad_norm": 0.4767564833164215,
      "learning_rate": 0.0001946432308014229,
      "loss": 1.2989,
      "step": 130
    },
    {
      "epoch": 0.08788449466415568,
      "grad_norm": 0.5223031044006348,
      "learning_rate": 0.00019422473320778407,
      "loss": 1.342,
      "step": 140
    },
    {
      "epoch": 0.09416195856873823,
      "grad_norm": 0.5819492936134338,
      "learning_rate": 0.00019380623561414523,
      "loss": 1.2332,
      "step": 150
    },
    {
      "epoch": 0.10043942247332077,
      "grad_norm": 0.5224732756614685,
      "learning_rate": 0.00019338773802050639,
      "loss": 1.335,
      "step": 160
    },
    {
      "epoch": 0.10671688637790333,
      "grad_norm": 0.5602819323539734,
      "learning_rate": 0.00019296924042686754,
      "loss": 1.2768,
      "step": 170
    },
    {
      "epoch": 0.11299435028248588,
      "grad_norm": 0.5439789891242981,
      "learning_rate": 0.0001925507428332287,
      "loss": 1.3653,
      "step": 180
    },
    {
      "epoch": 0.11927181418706842,
      "grad_norm": 0.5691562294960022,
      "learning_rate": 0.0001921322452395899,
      "loss": 1.1397,
      "step": 190
    },
    {
      "epoch": 0.12554927809165098,
      "grad_norm": 0.5623987913131714,
      "learning_rate": 0.00019171374764595105,
      "loss": 1.327,
      "step": 200
    },
    {
      "epoch": 0.1318267419962335,
      "grad_norm": 0.4990025758743286,
      "learning_rate": 0.0001912952500523122,
      "loss": 1.2959,
      "step": 210
    },
    {
      "epoch": 0.13810420590081607,
      "grad_norm": 0.5869375467300415,
      "learning_rate": 0.0001908767524586734,
      "loss": 1.3481,
      "step": 220
    },
    {
      "epoch": 0.14438166980539863,
      "grad_norm": 0.7071928977966309,
      "learning_rate": 0.00019045825486503452,
      "loss": 1.3288,
      "step": 230
    },
    {
      "epoch": 0.15065913370998116,
      "grad_norm": 0.5487568974494934,
      "learning_rate": 0.00019003975727139568,
      "loss": 1.3137,
      "step": 240
    },
    {
      "epoch": 0.15693659761456372,
      "grad_norm": 0.5254260897636414,
      "learning_rate": 0.00018962125967775687,
      "loss": 1.2077,
      "step": 250
    },
    {
      "epoch": 0.16321406151914628,
      "grad_norm": 0.6355239748954773,
      "learning_rate": 0.00018920276208411803,
      "loss": 1.3222,
      "step": 260
    },
    {
      "epoch": 0.1694915254237288,
      "grad_norm": 0.6866852045059204,
      "learning_rate": 0.0001887842644904792,
      "loss": 1.2311,
      "step": 270
    },
    {
      "epoch": 0.17576898932831136,
      "grad_norm": 0.5490036010742188,
      "learning_rate": 0.00018836576689684035,
      "loss": 1.1622,
      "step": 280
    },
    {
      "epoch": 0.18204645323289392,
      "grad_norm": 0.606478750705719,
      "learning_rate": 0.00018794726930320153,
      "loss": 1.2302,
      "step": 290
    },
    {
      "epoch": 0.18832391713747645,
      "grad_norm": 0.5532550811767578,
      "learning_rate": 0.0001875287717095627,
      "loss": 1.2107,
      "step": 300
    },
    {
      "epoch": 0.194601381042059,
      "grad_norm": 0.5585848093032837,
      "learning_rate": 0.00018711027411592382,
      "loss": 1.3093,
      "step": 310
    },
    {
      "epoch": 0.20087884494664154,
      "grad_norm": 0.5987712144851685,
      "learning_rate": 0.000186691776522285,
      "loss": 1.2486,
      "step": 320
    },
    {
      "epoch": 0.2071563088512241,
      "grad_norm": 0.5747607946395874,
      "learning_rate": 0.00018627327892864617,
      "loss": 1.1906,
      "step": 330
    },
    {
      "epoch": 0.21343377275580666,
      "grad_norm": 0.5857958793640137,
      "learning_rate": 0.00018585478133500733,
      "loss": 1.3232,
      "step": 340
    },
    {
      "epoch": 0.2197112366603892,
      "grad_norm": 0.6156118512153625,
      "learning_rate": 0.00018543628374136849,
      "loss": 1.3076,
      "step": 350
    },
    {
      "epoch": 0.22598870056497175,
      "grad_norm": 0.529983401298523,
      "learning_rate": 0.00018501778614772967,
      "loss": 1.1786,
      "step": 360
    },
    {
      "epoch": 0.2322661644695543,
      "grad_norm": 0.6434121131896973,
      "learning_rate": 0.00018459928855409083,
      "loss": 1.1651,
      "step": 370
    },
    {
      "epoch": 0.23854362837413684,
      "grad_norm": 0.6037746071815491,
      "learning_rate": 0.000184180790960452,
      "loss": 1.2013,
      "step": 380
    },
    {
      "epoch": 0.2448210922787194,
      "grad_norm": 0.5821081399917603,
      "learning_rate": 0.00018376229336681315,
      "loss": 1.2934,
      "step": 390
    },
    {
      "epoch": 0.25109855618330196,
      "grad_norm": 0.6177308559417725,
      "learning_rate": 0.0001833437957731743,
      "loss": 1.264,
      "step": 400
    },
    {
      "epoch": 0.2573760200878845,
      "grad_norm": 0.6003419756889343,
      "learning_rate": 0.00018292529817953547,
      "loss": 1.2523,
      "step": 410
    },
    {
      "epoch": 0.263653483992467,
      "grad_norm": 0.5575363636016846,
      "learning_rate": 0.00018250680058589665,
      "loss": 1.2351,
      "step": 420
    },
    {
      "epoch": 0.2699309478970496,
      "grad_norm": 0.6556302309036255,
      "learning_rate": 0.0001820883029922578,
      "loss": 1.3164,
      "step": 430
    },
    {
      "epoch": 0.27620841180163214,
      "grad_norm": 0.5866472125053406,
      "learning_rate": 0.00018166980539861897,
      "loss": 1.1167,
      "step": 440
    },
    {
      "epoch": 0.2824858757062147,
      "grad_norm": 0.5813003778457642,
      "learning_rate": 0.00018125130780498013,
      "loss": 1.3218,
      "step": 450
    },
    {
      "epoch": 0.28876333961079725,
      "grad_norm": 0.5293742418289185,
      "learning_rate": 0.0001808328102113413,
      "loss": 1.1309,
      "step": 460
    },
    {
      "epoch": 0.2950408035153798,
      "grad_norm": 0.5999680161476135,
      "learning_rate": 0.00018041431261770245,
      "loss": 1.1407,
      "step": 470
    },
    {
      "epoch": 0.3013182674199623,
      "grad_norm": 0.6984972357749939,
      "learning_rate": 0.0001799958150240636,
      "loss": 1.172,
      "step": 480
    },
    {
      "epoch": 0.3075957313245449,
      "grad_norm": 0.5807093381881714,
      "learning_rate": 0.0001795773174304248,
      "loss": 1.1185,
      "step": 490
    },
    {
      "epoch": 0.31387319522912743,
      "grad_norm": 0.6095670461654663,
      "learning_rate": 0.00017915881983678595,
      "loss": 1.2552,
      "step": 500
    },
    {
      "epoch": 0.32015065913371,
      "grad_norm": 0.6176935434341431,
      "learning_rate": 0.0001787403222431471,
      "loss": 1.1024,
      "step": 510
    },
    {
      "epoch": 0.32642812303829255,
      "grad_norm": 0.6117128133773804,
      "learning_rate": 0.0001783218246495083,
      "loss": 1.1322,
      "step": 520
    },
    {
      "epoch": 0.33270558694287505,
      "grad_norm": 0.5836730003356934,
      "learning_rate": 0.00017790332705586943,
      "loss": 1.1881,
      "step": 530
    },
    {
      "epoch": 0.3389830508474576,
      "grad_norm": 0.6919256448745728,
      "learning_rate": 0.0001774848294622306,
      "loss": 1.1781,
      "step": 540
    },
    {
      "epoch": 0.34526051475204017,
      "grad_norm": 0.6048715710639954,
      "learning_rate": 0.00017706633186859177,
      "loss": 1.2875,
      "step": 550
    },
    {
      "epoch": 0.35153797865662273,
      "grad_norm": 0.6428632736206055,
      "learning_rate": 0.00017664783427495293,
      "loss": 1.1812,
      "step": 560
    },
    {
      "epoch": 0.3578154425612053,
      "grad_norm": 0.5968446135520935,
      "learning_rate": 0.0001762293366813141,
      "loss": 1.0925,
      "step": 570
    },
    {
      "epoch": 0.36409290646578785,
      "grad_norm": 0.6082137823104858,
      "learning_rate": 0.00017581083908767525,
      "loss": 1.1327,
      "step": 580
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 0.658845067024231,
      "learning_rate": 0.00017539234149403644,
      "loss": 1.2199,
      "step": 590
    },
    {
      "epoch": 0.3766478342749529,
      "grad_norm": 0.5680555105209351,
      "learning_rate": 0.00017497384390039757,
      "loss": 1.1641,
      "step": 600
    },
    {
      "epoch": 0.38292529817953547,
      "grad_norm": 0.6168239116668701,
      "learning_rate": 0.00017455534630675873,
      "loss": 1.1907,
      "step": 610
    },
    {
      "epoch": 0.389202762084118,
      "grad_norm": 0.6627384424209595,
      "learning_rate": 0.0001741368487131199,
      "loss": 1.2232,
      "step": 620
    },
    {
      "epoch": 0.3954802259887006,
      "grad_norm": 0.6467877626419067,
      "learning_rate": 0.00017371835111948107,
      "loss": 1.1162,
      "step": 630
    },
    {
      "epoch": 0.4017576898932831,
      "grad_norm": 0.5411285758018494,
      "learning_rate": 0.00017329985352584223,
      "loss": 1.0502,
      "step": 640
    },
    {
      "epoch": 0.40803515379786565,
      "grad_norm": 0.6716124415397644,
      "learning_rate": 0.00017288135593220342,
      "loss": 1.1589,
      "step": 650
    },
    {
      "epoch": 0.4143126177024482,
      "grad_norm": 0.750100314617157,
      "learning_rate": 0.00017246285833856457,
      "loss": 1.1205,
      "step": 660
    },
    {
      "epoch": 0.42059008160703076,
      "grad_norm": 0.6240184307098389,
      "learning_rate": 0.0001720443607449257,
      "loss": 1.1092,
      "step": 670
    },
    {
      "epoch": 0.4268675455116133,
      "grad_norm": 0.7163110971450806,
      "learning_rate": 0.0001716258631512869,
      "loss": 1.2119,
      "step": 680
    },
    {
      "epoch": 0.4331450094161959,
      "grad_norm": 0.6634250283241272,
      "learning_rate": 0.00017120736555764805,
      "loss": 1.1628,
      "step": 690
    },
    {
      "epoch": 0.4394224733207784,
      "grad_norm": 0.5920068621635437,
      "learning_rate": 0.0001707888679640092,
      "loss": 1.181,
      "step": 700
    },
    {
      "epoch": 0.44569993722536094,
      "grad_norm": 0.685039758682251,
      "learning_rate": 0.00017037037037037037,
      "loss": 1.1402,
      "step": 710
    },
    {
      "epoch": 0.4519774011299435,
      "grad_norm": 0.618159294128418,
      "learning_rate": 0.00016995187277673156,
      "loss": 1.1517,
      "step": 720
    },
    {
      "epoch": 0.45825486503452606,
      "grad_norm": 0.6547849774360657,
      "learning_rate": 0.00016953337518309271,
      "loss": 1.112,
      "step": 730
    },
    {
      "epoch": 0.4645323289391086,
      "grad_norm": 0.5844735503196716,
      "learning_rate": 0.00016911487758945387,
      "loss": 1.2382,
      "step": 740
    },
    {
      "epoch": 0.4708097928436911,
      "grad_norm": 0.6468414664268494,
      "learning_rate": 0.00016869637999581503,
      "loss": 1.1144,
      "step": 750
    },
    {
      "epoch": 0.4770872567482737,
      "grad_norm": 0.6308430433273315,
      "learning_rate": 0.0001682778824021762,
      "loss": 1.2069,
      "step": 760
    },
    {
      "epoch": 0.48336472065285624,
      "grad_norm": 0.6815651655197144,
      "learning_rate": 0.00016785938480853735,
      "loss": 1.1049,
      "step": 770
    },
    {
      "epoch": 0.4896421845574388,
      "grad_norm": 0.5753756165504456,
      "learning_rate": 0.00016744088721489854,
      "loss": 1.1135,
      "step": 780
    },
    {
      "epoch": 0.49591964846202136,
      "grad_norm": 0.51607745885849,
      "learning_rate": 0.0001670223896212597,
      "loss": 1.1865,
      "step": 790
    },
    {
      "epoch": 0.5021971123666039,
      "grad_norm": 0.5711470246315002,
      "learning_rate": 0.00016660389202762085,
      "loss": 1.0901,
      "step": 800
    },
    {
      "epoch": 0.5084745762711864,
      "grad_norm": 0.5754309892654419,
      "learning_rate": 0.000166185394433982,
      "loss": 1.1554,
      "step": 810
    },
    {
      "epoch": 0.514752040175769,
      "grad_norm": 0.6652283072471619,
      "learning_rate": 0.00016576689684034317,
      "loss": 1.056,
      "step": 820
    },
    {
      "epoch": 0.5210295040803515,
      "grad_norm": 0.6516441702842712,
      "learning_rate": 0.00016534839924670433,
      "loss": 1.1357,
      "step": 830
    },
    {
      "epoch": 0.527306967984934,
      "grad_norm": 0.670681893825531,
      "learning_rate": 0.0001649299016530655,
      "loss": 1.1091,
      "step": 840
    },
    {
      "epoch": 0.5335844318895167,
      "grad_norm": 0.6143217086791992,
      "learning_rate": 0.00016451140405942668,
      "loss": 1.118,
      "step": 850
    },
    {
      "epoch": 0.5398618957940992,
      "grad_norm": 0.6466380953788757,
      "learning_rate": 0.00016409290646578783,
      "loss": 1.1217,
      "step": 860
    },
    {
      "epoch": 0.5461393596986818,
      "grad_norm": 0.7308236360549927,
      "learning_rate": 0.000163674408872149,
      "loss": 1.1323,
      "step": 870
    },
    {
      "epoch": 0.5524168236032643,
      "grad_norm": 0.6125099658966064,
      "learning_rate": 0.00016325591127851015,
      "loss": 1.1789,
      "step": 880
    },
    {
      "epoch": 0.5586942875078468,
      "grad_norm": 0.676082193851471,
      "learning_rate": 0.0001628374136848713,
      "loss": 1.1329,
      "step": 890
    },
    {
      "epoch": 0.5649717514124294,
      "grad_norm": 0.5871046781539917,
      "learning_rate": 0.00016241891609123247,
      "loss": 1.0751,
      "step": 900
    },
    {
      "epoch": 0.5712492153170119,
      "grad_norm": 0.635615885257721,
      "learning_rate": 0.00016200041849759363,
      "loss": 1.1082,
      "step": 910
    },
    {
      "epoch": 0.5775266792215945,
      "grad_norm": 0.6593565344810486,
      "learning_rate": 0.00016158192090395482,
      "loss": 1.0874,
      "step": 920
    },
    {
      "epoch": 0.583804143126177,
      "grad_norm": 0.6341730356216431,
      "learning_rate": 0.00016116342331031597,
      "loss": 1.0758,
      "step": 930
    },
    {
      "epoch": 0.5900816070307596,
      "grad_norm": 0.6925804615020752,
      "learning_rate": 0.00016074492571667713,
      "loss": 1.2282,
      "step": 940
    },
    {
      "epoch": 0.5963590709353421,
      "grad_norm": 0.6672784686088562,
      "learning_rate": 0.00016032642812303832,
      "loss": 1.0888,
      "step": 950
    },
    {
      "epoch": 0.6026365348399246,
      "grad_norm": 0.6547020673751831,
      "learning_rate": 0.00015990793052939948,
      "loss": 1.0958,
      "step": 960
    },
    {
      "epoch": 0.6089139987445072,
      "grad_norm": 0.6606735587120056,
      "learning_rate": 0.0001594894329357606,
      "loss": 1.2696,
      "step": 970
    },
    {
      "epoch": 0.6151914626490897,
      "grad_norm": 0.6772971749305725,
      "learning_rate": 0.0001590709353421218,
      "loss": 1.118,
      "step": 980
    },
    {
      "epoch": 0.6214689265536724,
      "grad_norm": 0.6363801956176758,
      "learning_rate": 0.00015865243774848295,
      "loss": 1.166,
      "step": 990
    },
    {
      "epoch": 0.6277463904582549,
      "grad_norm": 0.6266388297080994,
      "learning_rate": 0.0001582339401548441,
      "loss": 1.1045,
      "step": 1000
    },
    {
      "epoch": 0.6340238543628374,
      "grad_norm": 0.6218175888061523,
      "learning_rate": 0.00015781544256120527,
      "loss": 1.0408,
      "step": 1010
    },
    {
      "epoch": 0.64030131826742,
      "grad_norm": 0.5792235732078552,
      "learning_rate": 0.00015739694496756646,
      "loss": 1.104,
      "step": 1020
    },
    {
      "epoch": 0.6465787821720025,
      "grad_norm": 0.6294731497764587,
      "learning_rate": 0.00015697844737392762,
      "loss": 1.0829,
      "step": 1030
    },
    {
      "epoch": 0.6528562460765851,
      "grad_norm": 0.7403737902641296,
      "learning_rate": 0.00015655994978028875,
      "loss": 1.1151,
      "step": 1040
    },
    {
      "epoch": 0.6591337099811676,
      "grad_norm": 0.6450133919715881,
      "learning_rate": 0.00015614145218664994,
      "loss": 1.0481,
      "step": 1050
    },
    {
      "epoch": 0.6654111738857501,
      "grad_norm": 0.7045389413833618,
      "learning_rate": 0.0001557229545930111,
      "loss": 1.1781,
      "step": 1060
    },
    {
      "epoch": 0.6716886377903327,
      "grad_norm": 0.6846441626548767,
      "learning_rate": 0.00015530445699937225,
      "loss": 1.1366,
      "step": 1070
    },
    {
      "epoch": 0.6779661016949152,
      "grad_norm": 0.7629581093788147,
      "learning_rate": 0.00015488595940573344,
      "loss": 1.1758,
      "step": 1080
    },
    {
      "epoch": 0.6842435655994978,
      "grad_norm": 0.59344482421875,
      "learning_rate": 0.0001544674618120946,
      "loss": 0.9343,
      "step": 1090
    },
    {
      "epoch": 0.6905210295040803,
      "grad_norm": 0.7283827662467957,
      "learning_rate": 0.00015404896421845576,
      "loss": 1.0856,
      "step": 1100
    },
    {
      "epoch": 0.696798493408663,
      "grad_norm": 0.6444827914237976,
      "learning_rate": 0.00015363046662481692,
      "loss": 1.2752,
      "step": 1110
    },
    {
      "epoch": 0.7030759573132455,
      "grad_norm": 0.6085330843925476,
      "learning_rate": 0.00015321196903117807,
      "loss": 1.2076,
      "step": 1120
    },
    {
      "epoch": 0.709353421217828,
      "grad_norm": 0.6553382277488708,
      "learning_rate": 0.00015279347143753923,
      "loss": 1.0372,
      "step": 1130
    },
    {
      "epoch": 0.7156308851224106,
      "grad_norm": 0.7848677635192871,
      "learning_rate": 0.0001523749738439004,
      "loss": 1.1661,
      "step": 1140
    },
    {
      "epoch": 0.7219083490269931,
      "grad_norm": 0.7708779573440552,
      "learning_rate": 0.00015195647625026158,
      "loss": 1.068,
      "step": 1150
    },
    {
      "epoch": 0.7281858129315757,
      "grad_norm": 0.7204033136367798,
      "learning_rate": 0.00015153797865662274,
      "loss": 1.0772,
      "step": 1160
    },
    {
      "epoch": 0.7344632768361582,
      "grad_norm": 0.7171818017959595,
      "learning_rate": 0.0001511194810629839,
      "loss": 1.0819,
      "step": 1170
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 0.6792163252830505,
      "learning_rate": 0.00015070098346934508,
      "loss": 1.0478,
      "step": 1180
    },
    {
      "epoch": 0.7470182046453233,
      "grad_norm": 0.7011916041374207,
      "learning_rate": 0.00015028248587570621,
      "loss": 1.0659,
      "step": 1190
    },
    {
      "epoch": 0.7532956685499058,
      "grad_norm": 0.771087110042572,
      "learning_rate": 0.00014986398828206737,
      "loss": 1.0572,
      "step": 1200
    },
    {
      "epoch": 0.7595731324544884,
      "grad_norm": 0.7213941216468811,
      "learning_rate": 0.00014944549068842856,
      "loss": 1.1349,
      "step": 1210
    },
    {
      "epoch": 0.7658505963590709,
      "grad_norm": 0.7402423620223999,
      "learning_rate": 0.00014902699309478972,
      "loss": 1.0439,
      "step": 1220
    },
    {
      "epoch": 0.7721280602636534,
      "grad_norm": 0.7291733622550964,
      "learning_rate": 0.00014860849550115088,
      "loss": 0.9946,
      "step": 1230
    },
    {
      "epoch": 0.778405524168236,
      "grad_norm": 0.6595091223716736,
      "learning_rate": 0.00014818999790751204,
      "loss": 1.1717,
      "step": 1240
    },
    {
      "epoch": 0.7846829880728186,
      "grad_norm": 0.7239333987236023,
      "learning_rate": 0.00014777150031387322,
      "loss": 1.1148,
      "step": 1250
    },
    {
      "epoch": 0.7909604519774012,
      "grad_norm": 0.6787055730819702,
      "learning_rate": 0.00014735300272023435,
      "loss": 1.1886,
      "step": 1260
    },
    {
      "epoch": 0.7972379158819837,
      "grad_norm": 0.6657481789588928,
      "learning_rate": 0.0001469345051265955,
      "loss": 1.0916,
      "step": 1270
    },
    {
      "epoch": 0.8035153797865662,
      "grad_norm": 0.6373681426048279,
      "learning_rate": 0.0001465160075329567,
      "loss": 1.0502,
      "step": 1280
    },
    {
      "epoch": 0.8097928436911488,
      "grad_norm": 0.7223085761070251,
      "learning_rate": 0.00014609750993931786,
      "loss": 1.0622,
      "step": 1290
    },
    {
      "epoch": 0.8160703075957313,
      "grad_norm": 0.7469496130943298,
      "learning_rate": 0.00014567901234567902,
      "loss": 1.0787,
      "step": 1300
    },
    {
      "epoch": 0.8223477715003139,
      "grad_norm": 0.7159574627876282,
      "learning_rate": 0.0001452605147520402,
      "loss": 1.1834,
      "step": 1310
    },
    {
      "epoch": 0.8286252354048964,
      "grad_norm": 0.644140362739563,
      "learning_rate": 0.00014484201715840136,
      "loss": 1.0397,
      "step": 1320
    },
    {
      "epoch": 0.834902699309479,
      "grad_norm": 0.7259247899055481,
      "learning_rate": 0.0001444235195647625,
      "loss": 1.1355,
      "step": 1330
    },
    {
      "epoch": 0.8411801632140615,
      "grad_norm": 0.6653093099594116,
      "learning_rate": 0.00014400502197112368,
      "loss": 1.0427,
      "step": 1340
    },
    {
      "epoch": 0.847457627118644,
      "grad_norm": 0.7973365187644958,
      "learning_rate": 0.00014358652437748484,
      "loss": 1.1029,
      "step": 1350
    },
    {
      "epoch": 0.8537350910232266,
      "grad_norm": 0.6979200839996338,
      "learning_rate": 0.000143168026783846,
      "loss": 1.1607,
      "step": 1360
    },
    {
      "epoch": 0.8600125549278091,
      "grad_norm": 0.6326670050621033,
      "learning_rate": 0.00014274952919020716,
      "loss": 1.045,
      "step": 1370
    },
    {
      "epoch": 0.8662900188323918,
      "grad_norm": 0.625702440738678,
      "learning_rate": 0.00014233103159656834,
      "loss": 1.0339,
      "step": 1380
    },
    {
      "epoch": 0.8725674827369743,
      "grad_norm": 0.6416098475456238,
      "learning_rate": 0.0001419125340029295,
      "loss": 1.0018,
      "step": 1390
    },
    {
      "epoch": 0.8788449466415568,
      "grad_norm": 0.7704829573631287,
      "learning_rate": 0.00014149403640929063,
      "loss": 1.2115,
      "step": 1400
    },
    {
      "epoch": 0.8851224105461394,
      "grad_norm": 0.7153869271278381,
      "learning_rate": 0.00014107553881565182,
      "loss": 1.0938,
      "step": 1410
    },
    {
      "epoch": 0.8913998744507219,
      "grad_norm": 0.6864328980445862,
      "learning_rate": 0.00014065704122201298,
      "loss": 1.0394,
      "step": 1420
    },
    {
      "epoch": 0.8976773383553045,
      "grad_norm": 0.654288649559021,
      "learning_rate": 0.00014023854362837414,
      "loss": 1.1938,
      "step": 1430
    },
    {
      "epoch": 0.903954802259887,
      "grad_norm": 0.7491419315338135,
      "learning_rate": 0.0001398200460347353,
      "loss": 1.1091,
      "step": 1440
    },
    {
      "epoch": 0.9102322661644695,
      "grad_norm": 0.6868979334831238,
      "learning_rate": 0.00013940154844109648,
      "loss": 0.9598,
      "step": 1450
    },
    {
      "epoch": 0.9165097300690521,
      "grad_norm": 0.6900007724761963,
      "learning_rate": 0.00013898305084745764,
      "loss": 1.0869,
      "step": 1460
    },
    {
      "epoch": 0.9227871939736346,
      "grad_norm": 0.7423304319381714,
      "learning_rate": 0.0001385645532538188,
      "loss": 1.0446,
      "step": 1470
    },
    {
      "epoch": 0.9290646578782172,
      "grad_norm": 0.5945031642913818,
      "learning_rate": 0.00013814605566017996,
      "loss": 1.0632,
      "step": 1480
    },
    {
      "epoch": 0.9353421217827997,
      "grad_norm": 0.7075721621513367,
      "learning_rate": 0.00013772755806654112,
      "loss": 0.9744,
      "step": 1490
    },
    {
      "epoch": 0.9416195856873822,
      "grad_norm": 0.5863187909126282,
      "learning_rate": 0.00013730906047290228,
      "loss": 1.103,
      "step": 1500
    },
    {
      "epoch": 0.9478970495919649,
      "grad_norm": 0.7364603281021118,
      "learning_rate": 0.00013689056287926346,
      "loss": 1.1258,
      "step": 1510
    },
    {
      "epoch": 0.9541745134965474,
      "grad_norm": 0.7016408443450928,
      "learning_rate": 0.00013647206528562462,
      "loss": 1.0274,
      "step": 1520
    },
    {
      "epoch": 0.96045197740113,
      "grad_norm": 0.6275646090507507,
      "learning_rate": 0.00013605356769198578,
      "loss": 1.0329,
      "step": 1530
    },
    {
      "epoch": 0.9667294413057125,
      "grad_norm": 0.6753239631652832,
      "learning_rate": 0.00013563507009834694,
      "loss": 0.9847,
      "step": 1540
    },
    {
      "epoch": 0.9730069052102951,
      "grad_norm": 0.6707074642181396,
      "learning_rate": 0.0001352165725047081,
      "loss": 0.9565,
      "step": 1550
    },
    {
      "epoch": 0.9792843691148776,
      "grad_norm": 0.9146836996078491,
      "learning_rate": 0.00013479807491106926,
      "loss": 1.0973,
      "step": 1560
    },
    {
      "epoch": 0.9855618330194601,
      "grad_norm": 0.659652829170227,
      "learning_rate": 0.00013437957731743042,
      "loss": 0.9848,
      "step": 1570
    },
    {
      "epoch": 0.9918392969240427,
      "grad_norm": 0.7146149277687073,
      "learning_rate": 0.0001339610797237916,
      "loss": 1.0156,
      "step": 1580
    },
    {
      "epoch": 0.9981167608286252,
      "grad_norm": 0.6854270696640015,
      "learning_rate": 0.00013354258213015276,
      "loss": 1.008,
      "step": 1590
    },
    {
      "epoch": 1.0043942247332078,
      "grad_norm": 0.6003192067146301,
      "learning_rate": 0.00013312408453651392,
      "loss": 0.9894,
      "step": 1600
    },
    {
      "epoch": 1.0106716886377902,
      "grad_norm": 0.7241529226303101,
      "learning_rate": 0.0001327055869428751,
      "loss": 1.0649,
      "step": 1610
    },
    {
      "epoch": 1.0169491525423728,
      "grad_norm": 0.6407097578048706,
      "learning_rate": 0.00013228708934923624,
      "loss": 1.0458,
      "step": 1620
    },
    {
      "epoch": 1.0232266164469555,
      "grad_norm": 0.6208394765853882,
      "learning_rate": 0.0001318685917555974,
      "loss": 1.0023,
      "step": 1630
    },
    {
      "epoch": 1.029504080351538,
      "grad_norm": 0.6245283484458923,
      "learning_rate": 0.00013145009416195858,
      "loss": 0.9569,
      "step": 1640
    },
    {
      "epoch": 1.0357815442561205,
      "grad_norm": 0.7469456195831299,
      "learning_rate": 0.00013103159656831974,
      "loss": 1.0283,
      "step": 1650
    },
    {
      "epoch": 1.042059008160703,
      "grad_norm": 0.6283392310142517,
      "learning_rate": 0.0001306130989746809,
      "loss": 1.0558,
      "step": 1660
    },
    {
      "epoch": 1.0483364720652857,
      "grad_norm": 0.6579342484474182,
      "learning_rate": 0.00013019460138104206,
      "loss": 1.0348,
      "step": 1670
    },
    {
      "epoch": 1.054613935969868,
      "grad_norm": 0.6421488523483276,
      "learning_rate": 0.00012977610378740324,
      "loss": 1.0244,
      "step": 1680
    },
    {
      "epoch": 1.0608913998744507,
      "grad_norm": 0.6579471826553345,
      "learning_rate": 0.0001293576061937644,
      "loss": 0.9733,
      "step": 1690
    },
    {
      "epoch": 1.0671688637790333,
      "grad_norm": 0.643628716468811,
      "learning_rate": 0.00012893910860012554,
      "loss": 0.963,
      "step": 1700
    },
    {
      "epoch": 1.073446327683616,
      "grad_norm": 0.6279364228248596,
      "learning_rate": 0.00012852061100648672,
      "loss": 0.9363,
      "step": 1710
    },
    {
      "epoch": 1.0797237915881983,
      "grad_norm": 0.7169364094734192,
      "learning_rate": 0.00012810211341284788,
      "loss": 0.9956,
      "step": 1720
    },
    {
      "epoch": 1.086001255492781,
      "grad_norm": 0.651616632938385,
      "learning_rate": 0.00012768361581920904,
      "loss": 1.0894,
      "step": 1730
    },
    {
      "epoch": 1.0922787193973635,
      "grad_norm": 0.7480227947235107,
      "learning_rate": 0.00012726511822557023,
      "loss": 1.0015,
      "step": 1740
    },
    {
      "epoch": 1.098556183301946,
      "grad_norm": 0.7078272104263306,
      "learning_rate": 0.00012684662063193138,
      "loss": 0.9603,
      "step": 1750
    },
    {
      "epoch": 1.1048336472065285,
      "grad_norm": 0.7698292136192322,
      "learning_rate": 0.00012642812303829254,
      "loss": 1.0975,
      "step": 1760
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.7660356163978577,
      "learning_rate": 0.0001260096254446537,
      "loss": 1.0254,
      "step": 1770
    },
    {
      "epoch": 1.1173885750156938,
      "grad_norm": 0.7337697744369507,
      "learning_rate": 0.00012559112785101486,
      "loss": 0.9749,
      "step": 1780
    },
    {
      "epoch": 1.1236660389202762,
      "grad_norm": 0.6530495882034302,
      "learning_rate": 0.00012517263025737602,
      "loss": 0.9469,
      "step": 1790
    },
    {
      "epoch": 1.1299435028248588,
      "grad_norm": 0.7335605621337891,
      "learning_rate": 0.00012475413266373718,
      "loss": 1.0395,
      "step": 1800
    },
    {
      "epoch": 1.1362209667294414,
      "grad_norm": 0.7935880422592163,
      "learning_rate": 0.00012433563507009836,
      "loss": 0.9814,
      "step": 1810
    },
    {
      "epoch": 1.1424984306340238,
      "grad_norm": 0.6367689967155457,
      "learning_rate": 0.00012391713747645952,
      "loss": 1.1152,
      "step": 1820
    },
    {
      "epoch": 1.1487758945386064,
      "grad_norm": 0.5988656282424927,
      "learning_rate": 0.00012349863988282068,
      "loss": 1.0173,
      "step": 1830
    },
    {
      "epoch": 1.155053358443189,
      "grad_norm": 0.6528387665748596,
      "learning_rate": 0.00012308014228918184,
      "loss": 1.0382,
      "step": 1840
    },
    {
      "epoch": 1.1613308223477714,
      "grad_norm": 0.7551480531692505,
      "learning_rate": 0.000122661644695543,
      "loss": 1.0975,
      "step": 1850
    },
    {
      "epoch": 1.167608286252354,
      "grad_norm": 0.6961734294891357,
      "learning_rate": 0.00012224314710190416,
      "loss": 1.0291,
      "step": 1860
    },
    {
      "epoch": 1.1738857501569366,
      "grad_norm": 0.6932178139686584,
      "learning_rate": 0.00012182464950826533,
      "loss": 1.0235,
      "step": 1870
    },
    {
      "epoch": 1.180163214061519,
      "grad_norm": 0.6554912328720093,
      "learning_rate": 0.0001214061519146265,
      "loss": 0.9901,
      "step": 1880
    },
    {
      "epoch": 1.1864406779661016,
      "grad_norm": 0.8311054110527039,
      "learning_rate": 0.00012098765432098766,
      "loss": 1.1,
      "step": 1890
    },
    {
      "epoch": 1.1927181418706843,
      "grad_norm": 0.7572940587997437,
      "learning_rate": 0.00012056915672734884,
      "loss": 1.0725,
      "step": 1900
    },
    {
      "epoch": 1.1989956057752669,
      "grad_norm": 0.6659161448478699,
      "learning_rate": 0.00012015065913371,
      "loss": 1.0943,
      "step": 1910
    },
    {
      "epoch": 1.2052730696798493,
      "grad_norm": 0.760523796081543,
      "learning_rate": 0.00011973216154007114,
      "loss": 1.0631,
      "step": 1920
    },
    {
      "epoch": 1.2115505335844319,
      "grad_norm": 0.7421793341636658,
      "learning_rate": 0.00011931366394643231,
      "loss": 1.0091,
      "step": 1930
    },
    {
      "epoch": 1.2178279974890145,
      "grad_norm": 0.7063173651695251,
      "learning_rate": 0.00011889516635279347,
      "loss": 1.0146,
      "step": 1940
    },
    {
      "epoch": 1.2241054613935969,
      "grad_norm": 0.7936040163040161,
      "learning_rate": 0.00011847666875915464,
      "loss": 1.1205,
      "step": 1950
    },
    {
      "epoch": 1.2303829252981795,
      "grad_norm": 0.5886478424072266,
      "learning_rate": 0.0001180581711655158,
      "loss": 0.923,
      "step": 1960
    },
    {
      "epoch": 1.2366603892027621,
      "grad_norm": 0.6369995474815369,
      "learning_rate": 0.00011763967357187698,
      "loss": 1.0218,
      "step": 1970
    },
    {
      "epoch": 1.2429378531073447,
      "grad_norm": 0.7052714228630066,
      "learning_rate": 0.00011722117597823813,
      "loss": 0.9713,
      "step": 1980
    },
    {
      "epoch": 1.2492153170119271,
      "grad_norm": 0.5779129862785339,
      "learning_rate": 0.00011680267838459928,
      "loss": 0.9676,
      "step": 1990
    },
    {
      "epoch": 1.2554927809165097,
      "grad_norm": 0.6255979537963867,
      "learning_rate": 0.00011638418079096045,
      "loss": 1.0496,
      "step": 2000
    },
    {
      "epoch": 1.2617702448210923,
      "grad_norm": 0.7064706087112427,
      "learning_rate": 0.00011596568319732161,
      "loss": 1.0661,
      "step": 2010
    },
    {
      "epoch": 1.2680477087256747,
      "grad_norm": 0.683211088180542,
      "learning_rate": 0.00011554718560368278,
      "loss": 0.9842,
      "step": 2020
    },
    {
      "epoch": 1.2743251726302574,
      "grad_norm": 0.7057439684867859,
      "learning_rate": 0.00011512868801004396,
      "loss": 0.9978,
      "step": 2030
    },
    {
      "epoch": 1.28060263653484,
      "grad_norm": 0.627629280090332,
      "learning_rate": 0.00011471019041640511,
      "loss": 1.032,
      "step": 2040
    },
    {
      "epoch": 1.2868801004394226,
      "grad_norm": 0.6367796063423157,
      "learning_rate": 0.00011429169282276629,
      "loss": 0.9783,
      "step": 2050
    },
    {
      "epoch": 1.293157564344005,
      "grad_norm": 0.726061999797821,
      "learning_rate": 0.00011387319522912743,
      "loss": 0.9944,
      "step": 2060
    },
    {
      "epoch": 1.2994350282485876,
      "grad_norm": 0.7142391800880432,
      "learning_rate": 0.00011345469763548859,
      "loss": 1.0937,
      "step": 2070
    },
    {
      "epoch": 1.3057124921531702,
      "grad_norm": 0.6838685274124146,
      "learning_rate": 0.00011303620004184976,
      "loss": 1.0235,
      "step": 2080
    },
    {
      "epoch": 1.3119899560577526,
      "grad_norm": 0.732172429561615,
      "learning_rate": 0.00011261770244821092,
      "loss": 0.9669,
      "step": 2090
    },
    {
      "epoch": 1.3182674199623352,
      "grad_norm": 0.7668601274490356,
      "learning_rate": 0.0001121992048545721,
      "loss": 0.9712,
      "step": 2100
    },
    {
      "epoch": 1.3245448838669178,
      "grad_norm": 0.7525842785835266,
      "learning_rate": 0.00011178070726093325,
      "loss": 1.1188,
      "step": 2110
    },
    {
      "epoch": 1.3308223477715004,
      "grad_norm": 0.7587542533874512,
      "learning_rate": 0.00011136220966729443,
      "loss": 1.0777,
      "step": 2120
    },
    {
      "epoch": 1.3370998116760828,
      "grad_norm": 0.794150710105896,
      "learning_rate": 0.0001109437120736556,
      "loss": 1.0167,
      "step": 2130
    },
    {
      "epoch": 1.3433772755806654,
      "grad_norm": 0.7322328686714172,
      "learning_rate": 0.00011052521448001673,
      "loss": 0.9778,
      "step": 2140
    },
    {
      "epoch": 1.3496547394852478,
      "grad_norm": 0.6958417296409607,
      "learning_rate": 0.0001101067168863779,
      "loss": 0.8358,
      "step": 2150
    },
    {
      "epoch": 1.3559322033898304,
      "grad_norm": 0.6377500891685486,
      "learning_rate": 0.00010968821929273908,
      "loss": 0.9773,
      "step": 2160
    },
    {
      "epoch": 1.362209667294413,
      "grad_norm": 0.6809850931167603,
      "learning_rate": 0.00010926972169910023,
      "loss": 0.9628,
      "step": 2170
    },
    {
      "epoch": 1.3684871311989957,
      "grad_norm": 0.6270563006401062,
      "learning_rate": 0.00010885122410546141,
      "loss": 0.9903,
      "step": 2180
    },
    {
      "epoch": 1.3747645951035783,
      "grad_norm": 0.6804837584495544,
      "learning_rate": 0.00010843272651182257,
      "loss": 1.0514,
      "step": 2190
    },
    {
      "epoch": 1.3810420590081607,
      "grad_norm": 0.6313058137893677,
      "learning_rate": 0.00010801422891818374,
      "loss": 1.0643,
      "step": 2200
    },
    {
      "epoch": 1.3873195229127433,
      "grad_norm": 0.7516151666641235,
      "learning_rate": 0.00010759573132454488,
      "loss": 1.0973,
      "step": 2210
    },
    {
      "epoch": 1.3935969868173257,
      "grad_norm": 0.721291184425354,
      "learning_rate": 0.00010717723373090604,
      "loss": 1.0807,
      "step": 2220
    },
    {
      "epoch": 1.3998744507219083,
      "grad_norm": 0.7824544310569763,
      "learning_rate": 0.00010675873613726722,
      "loss": 1.0286,
      "step": 2230
    },
    {
      "epoch": 1.406151914626491,
      "grad_norm": 0.7278753519058228,
      "learning_rate": 0.00010634023854362837,
      "loss": 0.9777,
      "step": 2240
    },
    {
      "epoch": 1.4124293785310735,
      "grad_norm": 0.8517118692398071,
      "learning_rate": 0.00010592174094998955,
      "loss": 0.9735,
      "step": 2250
    },
    {
      "epoch": 1.418706842435656,
      "grad_norm": 0.7170505523681641,
      "learning_rate": 0.0001055032433563507,
      "loss": 1.007,
      "step": 2260
    },
    {
      "epoch": 1.4249843063402385,
      "grad_norm": 0.6595659852027893,
      "learning_rate": 0.00010508474576271188,
      "loss": 0.9071,
      "step": 2270
    },
    {
      "epoch": 1.4312617702448212,
      "grad_norm": 0.7543580532073975,
      "learning_rate": 0.00010466624816907302,
      "loss": 1.0606,
      "step": 2280
    },
    {
      "epoch": 1.4375392341494035,
      "grad_norm": 0.8915659785270691,
      "learning_rate": 0.00010424775057543418,
      "loss": 1.0913,
      "step": 2290
    },
    {
      "epoch": 1.4438166980539862,
      "grad_norm": 0.6226596832275391,
      "learning_rate": 0.00010382925298179535,
      "loss": 1.0586,
      "step": 2300
    },
    {
      "epoch": 1.4500941619585688,
      "grad_norm": 0.5797991752624512,
      "learning_rate": 0.00010341075538815653,
      "loss": 0.946,
      "step": 2310
    },
    {
      "epoch": 1.4563716258631514,
      "grad_norm": 0.7774348258972168,
      "learning_rate": 0.00010299225779451769,
      "loss": 1.0032,
      "step": 2320
    },
    {
      "epoch": 1.4626490897677338,
      "grad_norm": 0.7795247435569763,
      "learning_rate": 0.00010257376020087886,
      "loss": 1.1007,
      "step": 2330
    },
    {
      "epoch": 1.4689265536723164,
      "grad_norm": 0.7166784405708313,
      "learning_rate": 0.00010215526260724002,
      "loss": 1.067,
      "step": 2340
    },
    {
      "epoch": 1.475204017576899,
      "grad_norm": 0.6890245676040649,
      "learning_rate": 0.00010173676501360119,
      "loss": 0.8749,
      "step": 2350
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 0.7949071526527405,
      "learning_rate": 0.00010131826741996234,
      "loss": 1.0315,
      "step": 2360
    },
    {
      "epoch": 1.487758945386064,
      "grad_norm": 0.8094708919525146,
      "learning_rate": 0.0001008997698263235,
      "loss": 1.0886,
      "step": 2370
    },
    {
      "epoch": 1.4940364092906466,
      "grad_norm": 0.7193349599838257,
      "learning_rate": 0.00010048127223268467,
      "loss": 0.8835,
      "step": 2380
    },
    {
      "epoch": 1.5003138731952292,
      "grad_norm": 0.744175374507904,
      "learning_rate": 0.00010006277463904583,
      "loss": 0.995,
      "step": 2390
    },
    {
      "epoch": 1.5065913370998116,
      "grad_norm": 0.7476231455802917,
      "learning_rate": 9.9644277045407e-05,
      "loss": 0.9914,
      "step": 2400
    },
    {
      "epoch": 1.5128688010043942,
      "grad_norm": 0.7383854985237122,
      "learning_rate": 9.922577945176816e-05,
      "loss": 0.9735,
      "step": 2410
    },
    {
      "epoch": 1.5191462649089766,
      "grad_norm": 0.8346557021141052,
      "learning_rate": 9.880728185812932e-05,
      "loss": 1.0691,
      "step": 2420
    },
    {
      "epoch": 1.5254237288135593,
      "grad_norm": 0.7566072940826416,
      "learning_rate": 9.838878426449049e-05,
      "loss": 0.9959,
      "step": 2430
    },
    {
      "epoch": 1.5317011927181419,
      "grad_norm": 0.6202556490898132,
      "learning_rate": 9.797028667085165e-05,
      "loss": 0.9432,
      "step": 2440
    },
    {
      "epoch": 1.5379786566227245,
      "grad_norm": 0.7331885099411011,
      "learning_rate": 9.75517890772128e-05,
      "loss": 1.0238,
      "step": 2450
    },
    {
      "epoch": 1.544256120527307,
      "grad_norm": 0.7207855582237244,
      "learning_rate": 9.713329148357398e-05,
      "loss": 0.9959,
      "step": 2460
    },
    {
      "epoch": 1.5505335844318895,
      "grad_norm": 0.8413830995559692,
      "learning_rate": 9.671479388993514e-05,
      "loss": 0.9671,
      "step": 2470
    },
    {
      "epoch": 1.556811048336472,
      "grad_norm": 0.6904274225234985,
      "learning_rate": 9.62962962962963e-05,
      "loss": 0.989,
      "step": 2480
    },
    {
      "epoch": 1.5630885122410545,
      "grad_norm": 0.663164496421814,
      "learning_rate": 9.587779870265746e-05,
      "loss": 1.0091,
      "step": 2490
    },
    {
      "epoch": 1.569365976145637,
      "grad_norm": 0.6210168600082397,
      "learning_rate": 9.545930110901863e-05,
      "loss": 0.8921,
      "step": 2500
    },
    {
      "epoch": 1.5756434400502197,
      "grad_norm": 0.6323394179344177,
      "learning_rate": 9.50408035153798e-05,
      "loss": 0.9403,
      "step": 2510
    },
    {
      "epoch": 1.5819209039548023,
      "grad_norm": 0.8274778127670288,
      "learning_rate": 9.462230592174095e-05,
      "loss": 1.0039,
      "step": 2520
    },
    {
      "epoch": 1.588198367859385,
      "grad_norm": 0.6972327828407288,
      "learning_rate": 9.420380832810212e-05,
      "loss": 0.9147,
      "step": 2530
    },
    {
      "epoch": 1.5944758317639673,
      "grad_norm": 0.7473239898681641,
      "learning_rate": 9.378531073446328e-05,
      "loss": 0.9346,
      "step": 2540
    },
    {
      "epoch": 1.60075329566855,
      "grad_norm": 0.6532592177391052,
      "learning_rate": 9.336681314082445e-05,
      "loss": 1.0048,
      "step": 2550
    },
    {
      "epoch": 1.6070307595731324,
      "grad_norm": 0.6692841053009033,
      "learning_rate": 9.294831554718561e-05,
      "loss": 1.0488,
      "step": 2560
    },
    {
      "epoch": 1.613308223477715,
      "grad_norm": 0.7742165923118591,
      "learning_rate": 9.252981795354677e-05,
      "loss": 0.9486,
      "step": 2570
    },
    {
      "epoch": 1.6195856873822976,
      "grad_norm": 0.8424056768417358,
      "learning_rate": 9.211132035990794e-05,
      "loss": 1.0127,
      "step": 2580
    },
    {
      "epoch": 1.6258631512868802,
      "grad_norm": 0.6700634360313416,
      "learning_rate": 9.16928227662691e-05,
      "loss": 0.9782,
      "step": 2590
    },
    {
      "epoch": 1.6321406151914628,
      "grad_norm": 0.7035035490989685,
      "learning_rate": 9.127432517263026e-05,
      "loss": 1.0012,
      "step": 2600
    },
    {
      "epoch": 1.6384180790960452,
      "grad_norm": 0.7649480700492859,
      "learning_rate": 9.085582757899143e-05,
      "loss": 1.042,
      "step": 2610
    },
    {
      "epoch": 1.6446955430006276,
      "grad_norm": 0.6388869881629944,
      "learning_rate": 9.043732998535259e-05,
      "loss": 0.8509,
      "step": 2620
    },
    {
      "epoch": 1.6509730069052102,
      "grad_norm": 0.796817421913147,
      "learning_rate": 9.001883239171375e-05,
      "loss": 0.9922,
      "step": 2630
    },
    {
      "epoch": 1.6572504708097928,
      "grad_norm": 0.6295759081840515,
      "learning_rate": 8.960033479807492e-05,
      "loss": 1.0645,
      "step": 2640
    },
    {
      "epoch": 1.6635279347143754,
      "grad_norm": 0.7451056838035583,
      "learning_rate": 8.918183720443608e-05,
      "loss": 0.9213,
      "step": 2650
    },
    {
      "epoch": 1.669805398618958,
      "grad_norm": 0.6724480390548706,
      "learning_rate": 8.876333961079725e-05,
      "loss": 0.9115,
      "step": 2660
    },
    {
      "epoch": 1.6760828625235404,
      "grad_norm": 0.6724068522453308,
      "learning_rate": 8.83448420171584e-05,
      "loss": 0.921,
      "step": 2670
    },
    {
      "epoch": 1.682360326428123,
      "grad_norm": 0.7878633141517639,
      "learning_rate": 8.792634442351957e-05,
      "loss": 0.9473,
      "step": 2680
    },
    {
      "epoch": 1.6886377903327054,
      "grad_norm": 0.7017163038253784,
      "learning_rate": 8.750784682988074e-05,
      "loss": 0.9705,
      "step": 2690
    },
    {
      "epoch": 1.694915254237288,
      "grad_norm": 0.7194337248802185,
      "learning_rate": 8.708934923624189e-05,
      "loss": 0.9866,
      "step": 2700
    },
    {
      "epoch": 1.7011927181418707,
      "grad_norm": 0.7245289087295532,
      "learning_rate": 8.667085164260306e-05,
      "loss": 1.0458,
      "step": 2710
    },
    {
      "epoch": 1.7074701820464533,
      "grad_norm": 0.7063584327697754,
      "learning_rate": 8.625235404896422e-05,
      "loss": 0.9619,
      "step": 2720
    },
    {
      "epoch": 1.713747645951036,
      "grad_norm": 0.7000411152839661,
      "learning_rate": 8.583385645532539e-05,
      "loss": 0.9813,
      "step": 2730
    },
    {
      "epoch": 1.7200251098556183,
      "grad_norm": 0.7663021683692932,
      "learning_rate": 8.541535886168655e-05,
      "loss": 0.99,
      "step": 2740
    },
    {
      "epoch": 1.726302573760201,
      "grad_norm": 0.776628315448761,
      "learning_rate": 8.499686126804771e-05,
      "loss": 0.9342,
      "step": 2750
    },
    {
      "epoch": 1.7325800376647833,
      "grad_norm": 0.802300751209259,
      "learning_rate": 8.457836367440888e-05,
      "loss": 0.9103,
      "step": 2760
    },
    {
      "epoch": 1.738857501569366,
      "grad_norm": 0.6347243785858154,
      "learning_rate": 8.415986608077004e-05,
      "loss": 1.0132,
      "step": 2770
    },
    {
      "epoch": 1.7451349654739485,
      "grad_norm": 0.6982064247131348,
      "learning_rate": 8.37413684871312e-05,
      "loss": 0.9442,
      "step": 2780
    },
    {
      "epoch": 1.7514124293785311,
      "grad_norm": 0.7480707168579102,
      "learning_rate": 8.332287089349237e-05,
      "loss": 0.9186,
      "step": 2790
    },
    {
      "epoch": 1.7576898932831138,
      "grad_norm": 0.6729371547698975,
      "learning_rate": 8.290437329985353e-05,
      "loss": 1.0003,
      "step": 2800
    },
    {
      "epoch": 1.7639673571876961,
      "grad_norm": 0.7701439261436462,
      "learning_rate": 8.248587570621469e-05,
      "loss": 0.9397,
      "step": 2810
    },
    {
      "epoch": 1.7702448210922788,
      "grad_norm": 0.8375275135040283,
      "learning_rate": 8.206737811257585e-05,
      "loss": 0.8448,
      "step": 2820
    },
    {
      "epoch": 1.7765222849968612,
      "grad_norm": 0.6713117957115173,
      "learning_rate": 8.164888051893702e-05,
      "loss": 0.9339,
      "step": 2830
    },
    {
      "epoch": 1.7827997489014438,
      "grad_norm": 0.7425020933151245,
      "learning_rate": 8.12303829252982e-05,
      "loss": 0.9609,
      "step": 2840
    },
    {
      "epoch": 1.7890772128060264,
      "grad_norm": 0.6967371106147766,
      "learning_rate": 8.081188533165934e-05,
      "loss": 1.0792,
      "step": 2850
    },
    {
      "epoch": 1.795354676710609,
      "grad_norm": 0.7533058524131775,
      "learning_rate": 8.039338773802051e-05,
      "loss": 0.9485,
      "step": 2860
    },
    {
      "epoch": 1.8016321406151916,
      "grad_norm": 0.6905535459518433,
      "learning_rate": 7.997489014438167e-05,
      "loss": 0.9792,
      "step": 2870
    },
    {
      "epoch": 1.807909604519774,
      "grad_norm": 0.8177005648612976,
      "learning_rate": 7.955639255074284e-05,
      "loss": 0.9897,
      "step": 2880
    },
    {
      "epoch": 1.8141870684243564,
      "grad_norm": 0.6851024031639099,
      "learning_rate": 7.9137894957104e-05,
      "loss": 1.0104,
      "step": 2890
    },
    {
      "epoch": 1.820464532328939,
      "grad_norm": 0.8242319822311401,
      "learning_rate": 7.871939736346516e-05,
      "loss": 0.9566,
      "step": 2900
    },
    {
      "epoch": 1.8267419962335216,
      "grad_norm": 0.7438759207725525,
      "learning_rate": 7.830089976982633e-05,
      "loss": 0.9197,
      "step": 2910
    },
    {
      "epoch": 1.8330194601381042,
      "grad_norm": 0.6872438788414001,
      "learning_rate": 7.788240217618749e-05,
      "loss": 0.9863,
      "step": 2920
    },
    {
      "epoch": 1.8392969240426869,
      "grad_norm": 0.6776806712150574,
      "learning_rate": 7.746390458254865e-05,
      "loss": 0.962,
      "step": 2930
    },
    {
      "epoch": 1.8455743879472695,
      "grad_norm": 0.6414613723754883,
      "learning_rate": 7.704540698890982e-05,
      "loss": 1.1004,
      "step": 2940
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 0.7290835976600647,
      "learning_rate": 7.662690939527098e-05,
      "loss": 0.9763,
      "step": 2950
    },
    {
      "epoch": 1.8581293157564343,
      "grad_norm": 0.7501586079597473,
      "learning_rate": 7.620841180163214e-05,
      "loss": 0.9732,
      "step": 2960
    },
    {
      "epoch": 1.8644067796610169,
      "grad_norm": 0.7055151462554932,
      "learning_rate": 7.57899142079933e-05,
      "loss": 1.0116,
      "step": 2970
    },
    {
      "epoch": 1.8706842435655995,
      "grad_norm": 0.8125553131103516,
      "learning_rate": 7.537141661435447e-05,
      "loss": 0.9627,
      "step": 2980
    },
    {
      "epoch": 1.876961707470182,
      "grad_norm": 0.7260874509811401,
      "learning_rate": 7.495291902071564e-05,
      "loss": 0.8459,
      "step": 2990
    },
    {
      "epoch": 1.8832391713747647,
      "grad_norm": 0.6567785143852234,
      "learning_rate": 7.453442142707679e-05,
      "loss": 0.9775,
      "step": 3000
    },
    {
      "epoch": 1.889516635279347,
      "grad_norm": 0.8252705931663513,
      "learning_rate": 7.411592383343796e-05,
      "loss": 1.065,
      "step": 3010
    },
    {
      "epoch": 1.8957940991839297,
      "grad_norm": 0.7368611693382263,
      "learning_rate": 7.369742623979912e-05,
      "loss": 0.9138,
      "step": 3020
    },
    {
      "epoch": 1.902071563088512,
      "grad_norm": 0.6803000569343567,
      "learning_rate": 7.327892864616028e-05,
      "loss": 0.9336,
      "step": 3030
    },
    {
      "epoch": 1.9083490269930947,
      "grad_norm": 0.7635763883590698,
      "learning_rate": 7.286043105252145e-05,
      "loss": 0.9738,
      "step": 3040
    },
    {
      "epoch": 1.9146264908976773,
      "grad_norm": 0.7305115461349487,
      "learning_rate": 7.244193345888261e-05,
      "loss": 0.8895,
      "step": 3050
    },
    {
      "epoch": 1.92090395480226,
      "grad_norm": 0.5934025645256042,
      "learning_rate": 7.202343586524378e-05,
      "loss": 0.9573,
      "step": 3060
    },
    {
      "epoch": 1.9271814187068426,
      "grad_norm": 0.7836874723434448,
      "learning_rate": 7.160493827160494e-05,
      "loss": 0.9461,
      "step": 3070
    },
    {
      "epoch": 1.933458882611425,
      "grad_norm": 0.7988609671592712,
      "learning_rate": 7.11864406779661e-05,
      "loss": 0.9549,
      "step": 3080
    },
    {
      "epoch": 1.9397363465160076,
      "grad_norm": 0.7216009497642517,
      "learning_rate": 7.076794308432727e-05,
      "loss": 0.9431,
      "step": 3090
    },
    {
      "epoch": 1.94601381042059,
      "grad_norm": 0.7198189496994019,
      "learning_rate": 7.034944549068843e-05,
      "loss": 0.8438,
      "step": 3100
    },
    {
      "epoch": 1.9522912743251726,
      "grad_norm": 0.7556623220443726,
      "learning_rate": 6.993094789704959e-05,
      "loss": 0.9724,
      "step": 3110
    },
    {
      "epoch": 1.9585687382297552,
      "grad_norm": 0.724575400352478,
      "learning_rate": 6.951245030341076e-05,
      "loss": 0.9148,
      "step": 3120
    },
    {
      "epoch": 1.9648462021343378,
      "grad_norm": 0.646049976348877,
      "learning_rate": 6.909395270977192e-05,
      "loss": 0.8773,
      "step": 3130
    },
    {
      "epoch": 1.9711236660389204,
      "grad_norm": 0.749110221862793,
      "learning_rate": 6.867545511613308e-05,
      "loss": 1.0309,
      "step": 3140
    },
    {
      "epoch": 1.9774011299435028,
      "grad_norm": 0.7617254257202148,
      "learning_rate": 6.825695752249424e-05,
      "loss": 1.0986,
      "step": 3150
    },
    {
      "epoch": 1.9836785938480854,
      "grad_norm": 0.6587416529655457,
      "learning_rate": 6.783845992885541e-05,
      "loss": 1.0085,
      "step": 3160
    },
    {
      "epoch": 1.9899560577526678,
      "grad_norm": 0.7511833310127258,
      "learning_rate": 6.741996233521659e-05,
      "loss": 0.9077,
      "step": 3170
    },
    {
      "epoch": 1.9962335216572504,
      "grad_norm": 0.8248170018196106,
      "learning_rate": 6.700146474157773e-05,
      "loss": 0.9994,
      "step": 3180
    }
  ],
  "logging_steps": 10,
  "max_steps": 4779,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.2106468424823603e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
