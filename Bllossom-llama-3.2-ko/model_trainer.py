import torch
import mlflow
import json
import os
#import deepspeed
from transformers import Trainer, TrainingArguments
from accelerate import Accelerator 
#from deepspeed_config import get_deepspeed_config 

def train_model(experiment_name, model, tokenizer, tokenized_train_dataset, tokenized_validation_dataset):
    # MLflow ì‹¤í—˜ ì„¤ì •
    mlflow.set_experiment(experiment_name)

    with mlflow.start_run():  # MLflow ì‹¤í–‰ ì‹œì‘
        # MLflowì— í•™ìŠµ íŒŒë¼ë¯¸í„° ê¸°ë¡
        hyperparams = {
            "learning_rate": 2e-4,
            "batch_size": 4,
            "num_train_epochs": 3
        }
        mlflow.log_params(hyperparams)

        os.makedirs(experiment_name, exist_ok=True)
        with open(f"{experiment_name}/config.json", "w") as f:
            json.dump(hyperparams, f)
    
        # DeepSpeed ì„¤ì • ë¡œë“œ (ë³„ë„ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´)
        #ds_config = get_deepspeed_config(
            #batch_size=hyperparams["batch_size"],
            #gradient_accumulation_steps=2
        #)

        # TrainingArguments ì„¤ì •
        training_args = TrainingArguments(
            output_dir=experiment_name,
            per_device_train_batch_size=hyperparams["batch_size"],   # GPU VRAM ìµœì í™”
            gradient_accumulation_steps=2,  # ì‘ì€ ë°°ì¹˜ë¡œ í° ë°°ì¹˜ íš¨ê³¼
            learning_rate=hyperparams["learning_rate"],
            num_train_epochs=hyperparams["num_train_epochs"],
            logging_dir=f"{experiment_name}/logs",
            logging_steps=10,
            save_strategy="epoch",
            fp16=True, # AMP ì‚¬ìš©
            optim="adamw_torch",
            #deepspeed=ds_config,
        )

        # DeepSpeed ëª¨ë¸ ì´ˆê¸°í™”
        #model, optimizer, _, _ = deepspeed.initialize(
            #model=model,
            #model_parameters=model.parameters(),
            #config=ds_config
        #)

        # Trainer ì„¤ì •
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_train_dataset,
            eval_dataset=tokenized_validation_dataset 
        )

        print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        trainer.train()
        print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

        # í•™ìŠµ í›„ ì„±ëŠ¥ í‰ê°€
        train_metrics = trainer.evaluate()
        print(f"ğŸ“Š í•™ìŠµ í‰ê°€ ê²°ê³¼: {train_metrics}")
        mlflow.log_metrics(train_metrics)

        # ëª¨ë¸ ì €ì¥
        model.save_pretrained(f"{experiment_name}/finetuned_model")
        tokenizer.save_pretrained(f"{experiment_name}/finetuned_tokenizer")

        # ëª¨ë¸ì„ AMP ì ìš© í•´ì œ í›„ ì €ì¥
        accelerator = Accelerator()
        # ëª¨ë¸ì„ FP32ë¡œ ë³€í™˜ í›„ ì €ì¥
        unwrapped_model = accelerator.unwrap_model(model).to(torch.float32)

        model_save_path = f"{experiment_name}/finetuned_model.pt"
        torch.save(unwrapped_model.state_dict(), model_save_path)

        mlflow.log_artifact(model_save_path)

        mlflow.log_artifact(f"{experiment_name}/config.json")  
        mlflow.log_artifact(f"{experiment_name}/logs")  

        torch.cuda.empty_cache()

    return model